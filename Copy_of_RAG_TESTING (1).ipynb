{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8rDQY0cr2Ma",
        "outputId": "8a74c503-006d-40e0-a4ec-edc8928b2697",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.62-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core==0.10.62 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.62-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.27 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.29-py3-none-any.whl.metadata (650 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.32-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.62->llama-index) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (3.10.1)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.62->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.62->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.62->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (2024.6.1)\n",
            "Collecting httpx (from llama-index-core==0.10.62->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (1.26.4)\n",
            "Collecting openai>=1.1.0 (from llama-index-core==0.10.62->llama-index)\n",
            "  Downloading openai-1.40.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core==0.10.62->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.62->llama-index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.62->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.62->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index)\n",
            "  Downloading llama_cloud-0.0.12-py3-none-any.whl.metadata (751 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.62->llama-index) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.62->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.62->llama-index) (24.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.62->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.62->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.62->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.62->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.62->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.62->llama-index) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core==0.10.62->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.62->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.62->llama-index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core==0.10.62->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.62->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.62->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.62->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.62->llama-index) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.1.0->llama-index-core==0.10.62->llama-index)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.62->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.62->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.62->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.62->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.62->llama-index)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.62->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.62->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.62->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.62->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.62->llama-index) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.62->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.10.62-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_core-0.10.62-py3-none-any.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\n",
            "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.1.29-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.1.32-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_cloud-0.0.12-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
            "Downloading openai-1.40.1-py3-none-any.whl (360 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.4/360.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: striprtf, dirtyjson, tenacity, pypdf, mypy-extensions, marshmallow, jiter, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 llama-cloud-0.0.12 llama-index-0.10.62 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.62 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.29 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.32 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.9 marshmallow-3.21.3 mypy-extensions-1.0.0 openai-1.40.1 pypdf-4.3.1 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OkE-klvsIiz",
        "outputId": "31e52f4b-e2b0-4bb0-aa12-b2033f989543",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSSHMHuVsQV6",
        "outputId": "f377dbc2-8242-46c6-dcc7-98ab30015c91",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl.metadata (769 bytes)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.23.5)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (0.10.62)\n",
            "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.1)\n",
            "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
            "  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.31)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.40.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (9.4.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.42.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.3.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.5.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, minijinja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers, llama-index-embeddings-huggingface\n",
            "Successfully installed llama-index-embeddings-huggingface-0.2.2 minijinja-2.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, Document\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "import os\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.node_parser import HTMLNodeParser\n",
        "import re\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator\n",
        "from llama_index.core import Response\n",
        "import pandas as pd\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding"
      ],
      "metadata": {
        "id": "5ktnB99qhNhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY=''\n",
        "Settings.llm = OpenAI(temperature=0.0, model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\n",
        "Settings.embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY)\n",
        "# Settings.embed_model = HuggingFaceEmbedding(\n",
        "#     model_name=\"BAAI/bge-small-en-v1.5\"\n",
        "# )"
      ],
      "metadata": {
        "id": "89kYdPEpsrzZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_dataset = {\n",
        "    \"https://aws.amazon.com/what-is/reinforcement-learning/\": \"We use essential cookies and similar tools that are necessary to provide our site and services. We use performance cookies to collect anonymous statistics so we can understand how customers use our site and make improvements. Essential cookies cannot be deactivated, but you can click “Customize cookies” to decline performance cookies. If you agree, AWS and approved third parties will also use cookies to provide useful site features, remember your preferences, and display relevant content, including relevant advertising. To continue without accepting these cookies, click “Continue without accepting.” To make more detailed choices or learn more, click “Customize cookies.” Essential cookies are necessary to provide our site and services and cannot be deactivated. They are usually set in response to your actions on the site, such as setting your privacy preferences, signing in, or filling in forms. Performance cookies provide anonymous statistics about how customers navigate our site so we can improve site experience and performance. Approved third parties may perform analytics on our behalf, but they cannot use the data for their own purposes. Functional cookies help us provide useful site features, remember your preferences, and display relevant content. Approved third parties may set these cookies to provide certain site features. If you do not allow these cookies, then some or all of these services may not function properly. Advertising cookies may be set through our site by us or our advertising partners and help us deliver relevant marketing content. If you do not allow these cookies, you will experience less relevant advertising. Blocking some types of cookies may impact your experience of our sites. You may review and change your choices at any time by clicking Cookie preferences in the footer of this site. We and selected third-parties use cookies or similar technologies as specified in the AWS Cookie Notice. We will only store essential cookies at this time, because we were unable to save your cookie preferences.If you want to change your cookie preferences, try again later using the link in the AWS console footer, or contact support if the problem persists. Explore AWS Skill Builder | Access hundreds of free digital courses, wherever, whenever you want » Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals. Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored. RL algorithms use a reward-and-punishment paradigm as they process data. They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes. The algorithms are also capable of delayed gratification. The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments. There are many benefits to using reinforcement learning (RL). However, these three often stand out. RL algorithms can be used in complex environments with many rules and dependencies. In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment. Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results. In traditional ML algorithms, humans must label data pairs to direct the algorithm. When you use an RL algorithm, this isn’t necessary. It learns by itself. At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections. RL inherently focuses on long-term reward maximization, which makes it apt for scenarios where actions have prolonged consequences. It is particularly well-suited for real-world situations where feedback isn't immediately available for every step, since it can learn from delayed rewards. For example, decisions about energy consumption or storage might have long-term consequences. RL can be used to optimize long-term energy efficiency and cost. With appropriate architectures, RL agents can also generalize their learned strategies across similar but not identical tasks. Reinforcement learning (RL) can be applied to a wide range of real-world use cases. We give some examples next. In applications like recommendation systems, RL can customize suggestions to individual users based on their interactions. This leads to more personalized experiences. For example, an application may display ads to a user based on some demographic information. With each ad interaction, the application learns which ads to display to the user to optimize product sales. Traditional optimization methods solve problems by evaluating and comparing possible solutions based on certain criteria. In contrast, RL introduces learning from interactions to find the best or close-to-best solutions over time. For example, a cloud spend optimizing system uses RL to adjust to fluctuating resource needs and choose optimal instance types, quantities, and configurations. It makes decisions based on factors like current and available cloud infrastructure, spending, and utilization. The dynamics of financial markets are complex, with statistical properties that change over time. RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts. For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards. It dynamically creates a value function and develops a strategy to maximize profits. The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology. For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell. Soon, the child learns which combination of activities results in the end reward. An RL algorithm mimics a similar learning process. It tries different activities to learn the associated negative and positive values to achieve the end reward outcome. In reinforcement learning, there are a few key concepts to familiarize yourself with: Reinforcement learning is based on the Markov decision process, a mathematical modeling of decision-making that uses discrete time steps. At every step, the agent takes a new action that results in a new environment state. Similarly, the current state is attributed to the sequence of previous actions. Through trial and error in moving through the environment, the agent builds a set of if-then rules or policies. The policies help it decide which action to take next for optimal cumulative reward. The agent must also choose between further environment exploration to learn new state-action rewards or select known high-reward actions from a given state. This is called the exploration-exploitation trade-off . There are various algorithms used in reinforcement learning (RL)—such as Q-learning, policy gradient methods, Monte Carlo methods, and temporal difference learning. Deep RL is the application of deep neural networks to reinforcement learning. One example of a deep RL algorithm is Trust Region Policy Optimization (TRPO). All these algorithms can be grouped into two broad categories. Model-based RL is typically used when environments are well-defined and unchanging and where real-world environment testing is difficult. The agent first builds an internal representation (model) of the environment. It uses this process to build this model: Once the model is complete, the agent simulates action sequences based on the probability of optimal cumulative rewards. It then further assigns values to the action sequences themselves. The agent thus develops different strategies within the environment to achieve the desired end goal. Consider a robot learning to navigate a new building to reach a specific room. Initially, the robot explores freely and builds an internal model (or map) of the building. For instance, it might learn that it encounters an elevator after moving forward 10 meters from the main entrance. Once it builds the map, it can build a series of shortest-path sequences between different locations it visits frequently in the building. Model-free RL is best to use when the environment is large, complex, and not easily describable. It’s also ideal when the environment is unknown and changing, and environment-based testing does not come with significant downsides. The agent doesn’t build an internal model of the environment and its dynamics. Instead, it uses a trial-and-error approach within the environment. It scores and notes state-action pairs—and sequences of state-action pairs—to develop a policy. Consider a self-driving car that needs to navigate city traffic. Roads, traffic patterns, pedestrian behavior, and countless other factors can make the environment highly dynamic and complex. AI teams train the vehicle in a simulated environment in the initial stages. The vehicle takes actions based on its current state and receives rewards or penalties. Over time, by driving millions of miles in different virtual scenarios, the vehicle learns which actions are best for each state without explicitly modeling the entire traffic dynamics. When introduced in the real world, the vehicle uses the learned policy but continues to refine it with new data. While supervised learning, unsupervised learning, and reinforcement learning (RL) are all ML algorithms in the field of AI, there are distinctions between the three. Read about supervised and unsupervised learning » In supervised learning, you define both the input and the expected associated output. For instance, you can provide a set of images labeled dogs or cats, and the algorithm is then expected to identify a new animal image as a dog or cat. Supervised learning algorithms learn patterns and relationships between the input and output pairs. Then, they predict outcomes based on new input data. It requires a supervisor, typically a human, to label each data record in a training data set with an output. In contrast, RL has a well-defined end goal in the form of a desired result but no supervisor to label associated data in advance. During training, instead of trying to map inputs with known outputs, it maps inputs with possible outcomes. By rewarding desired behaviors, you give weightage to the best outcomes. Unsupervised learning algorithms receive inputs with no specified outputs during the training process. They find hidden patterns and relationships within the data using statistical means. For instance, you could provide a set of documents, and the algorithm may group them into categories it identifies based on the words in the text. You do not get any specific outcomes; they fall within a range. Conversely, RL has a predetermined end goal. While it takes an exploratory approach, the explorations are continuously validated and improved to increase the probability of reaching the end goal. It can teach itself to reach very specific outcomes. While reinforcement learning (RL) applications can potentially change the world, it may not be easy to deploy these algorithms. Experimenting with real-world reward and punishment systems may not be practical. For instance, testing a drone in the real world without testing in a simulator first would lead to significant numbers of broken aircraft. Real-world environments change often, significantly, and with limited warning. It can make it harder for the algorithm to be effective in practice. Like any field of science, data science also looks at conclusive research and findings to establish standards and procedures. Data scientists prefer knowing how a specific conclusion was reached for provability and replication. With complex RL algorithms, the reasons why a particular sequence of steps was taken may be difficult to ascertain. Which actions in a sequence were the ones that led to the optimal end result? This can be difficult to deduce, which causes implementation challenges. Amazon Web Services (AWS) has many offerings that help you develop, train, and deploy reinforcement learning (RL) algorithms for real-world applications. With Amazon SageMaker , developers and data scientists can quickly and easily develop scalable RL models. Combine a deep learning framework (like TensorFlow or Apache MXNet), an RL toolkit (like RL Coach or RLlib), and an environment to mimic a real-world scenario. You can use it to create and test your model. With AWS RoboMaker , developers can run, scale, and automate simulation with RL algorithms for robotics without any infrastructure requirements. Get hands-on experience with AWS DeepRacer , the fully autonomous 1/18th scale race car. It boasts a fully configured cloud environment that you can use to train your RL models and neural network configurations. Get started with reinforcement learning on AWS by creating an account today.\",\n",
        "\n",
        "\n",
        "    \"https://github.com/Vidhi1290/LLM---Detect-AI-Generated-Text\": \"© 2024 GitHub, Inc. We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Create a list to organize your starred repositories. Name . 32 remaining Description . 160 remaining type to add emoji to the name or description. Couldn't load subscription status. Retry Forks could not be loaded This will remove {{ repoNameWithOwner }} from the {{ listsWithCount }} that it's been added to. AI-Generated Text Detection: A BERT-powered solution for accurately identifying AI-generated text. Seamlessly integrated, highly accurate, and user-friendly.🚀 Welcome to our AI-Generated Text Detection project! In this repository, we present a robust solution for detecting AI-generated text using BERT, a cutting-edge natural language processing model. Whether you're a researcher, developer, or a curious enthusiast, this project empowers you to explore, understand, and combat AI-generated content effectively. AI-generated content is becoming increasingly sophisticated, making it challenging to distinguish between genuine and computer-generated text. Our project aims to tackle this issue by leveraging the power of BERT (Bidirectional Encoder Representations from Transformers) to identify and flag AI-generated text segments. Whether you're dealing with chatbots, articles, or social media posts, our solution offers accurate detection, ensuring the authenticity of digital content. Follow these simple steps to get started with our AI-Generated Text Detection tool: Our solution follows a comprehensive approach to AI-generated text detection: Data Preprocessing: We clean and preprocess the textual data, removing noise and irrelevant information to enhance the accuracy of our model. BERT Tokenization: Leveraging the BERT tokenizer, we encode the preprocessed text, preparing it for input into our detection model. Model Training: Using a BERT-based sequence classification model, we train the system to distinguish between genuine and AI-generated text with a high degree of accuracy. Predictions: Once trained, the model generates predictions for test data, highlighting potential AI-generated content segments. Result Analysis: The results are saved in a CSV file, allowing users to review and analyze the detected segments along with their confidence scores. We welcome contributions from the community! Whether you're a seasoned developer, a data science enthusiast, or a domain expert, your insights and expertise can enhance our project. 🚀 Connect With Me: If you find this project interesting or helpful, don't hesitate to follow me for more exciting updates and projects! Let's learn and grow together! 🌟 AI-Generated Text Detection: A BERT-powered solution for accurately identifying AI-generated text. Seamlessly integrated, highly accurate, and user-friendly.🚀\",\n",
        "\n",
        "\n",
        "    \"https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca\": \"Sign up Sign in Sign up Sign in Dipika Baad Follow The Startup -- 1 Listen Share Background to Word Embeddings and Implementing Sentiment Classification on Yelp Restaurant Review Text Data using Word2Vec. How the word embeddings are learned and used for different tasks will be explored in the beginning followed by using Word2Vec vectors for doing sentiment classification on Yelp Restaurant Review Dataset. In my previous posts of Sentiment Classification using BOW and Sentiment Classication using TFIDF , I have covered the topics of preprocessing the text and loading the data. This will be similar to those posts and we can quickly compare the results to those at the end as well. This post is one step further where little more complex method for representing the text is used to get the vectors for documents which tries to capture more than just the word information/importance. Let’s begin with loading the data. Yelp restaurant review dataset can be downloaded from their site and the format of the data present there is JSON. The data provided is actually not in correct json format readable for python. Each row is dictionary but for it to be a valid json format, a square bracket should be at the start and end of the file with , being added at end of each row. Define the INPUT_FOLDER as folder path in your local directory where yelp review.json file is present. Declare OUTPUT_FOLDER as a path where you want to write the output from the following function. Loading of json data and writing the top 100,000 rows is done in the following function: Once the above function has been run, you are ready to load it in pandas dataframe for the next steps. For the experiment, only small amount of data is taken so that it can be run faster to see the results. After the data is loaded, new column for sentiment indication is created. It is not always the situation that some column with the prediction label you want to do is present in the original dataset. This can be a derived column in most of the cases. For this case, stars column in the data is used to derive sentiment. Output: After the data is available, mapping from stars to sentiment is done and distribution for each sentiment is plotted. Output: Once that is done, number of rows for each sentiment is checked. Sentiment Classes are as follows: Number of rows are not equally distributed across these three sentiments. In this post, problem of imbalanced classes won’t be dealt that is why, simple function to retrieve the top few records for each sentiment is written. In this example, top_n is 10000 which means total of 30,000 records will be taken. Output: Preprocessing involves many steps like tokenization, removing stop words, stemming/lemmatization etc. These commonly used techniques were explained in detail in my previous post of BOW . Here, only the necessary steps are explained in the next phase. Why do you need to preprocess this text? — Not all the information is useful in making predictions or doing classifications. Reducing the number of words will reduce the input dimension to your model. The way the language is written, it contains lot of information which is grammar specific. Thus when converting to numeric format, word specific characteristics like capitalisation, punctuations, suffixes/prefixes etc. are redundant. Cleaning the data in a way that similar words map to single word and removing the grammar relevant information from text can tremendously reduce the vocabulary. Which methods to apply and which ones to skip depends on the problem at hand. Stop words are the words which are commonly used and removed from the sentence as pre-step in different Natural Language Processing (NLP) tasks. Example of stop words are: ‘a’, ‘an’, ‘the’, ‘this’, ‘not’ etc. Every tool uses a bit different set of stop words list that it removes but this technique is avoided in cases where phrase structure matters like in this case of Sentiment Analysis. Example of removing stop words: Output: As it can be seen from the output, removal of stop words removes necessary words required to get the sentiment and sometimes it can totally change the meaning of the sentence. In the examples printed by above piece of code, it is clear that it can convert a negative statement into positive sentence. Thus, this step is skipped for Sentiment Classification. Tokenization is the process in which the sentence/text is split into array of words called tokens. This helps to do transformations on each words separately and this is also required to transform words to numbers. There are different ways of performing tokenization. I have explained these ways in my previous post under Tokenization section, so if you are interested you can check it out. Gensim’s simple_preprocess allows you to convert text to lower case and remove punctuations. It has min and max length parameters as well which help to filter out rare words and most commonly words which will fall in that range of lengths. Here, simple_preprocess is used to get the tokens for the dataframe as it does most of the preprocessing already for us. Let’s apply this method to get the tokens for the dataframe: Output: 3. Stemming Stemming process reduces the words to its’ root word. Unlike Lemmatization which uses grammar rules and dictionary for mapping words to root form, stemming simply removes suffixes/prefixes. Stemming is widely used in the application of SEOs, Web search results, and information retrieval since as long as the root matches in the text somewhere it helps to retrieve all the related documents in the search. There are different algorithms used to do the stemming. PorterStammer(1979), LancasterStammer (1990), and SnowballStemmer ( can add custom rules). NLTK or Gensim package can be used for implementing these algorithms for stemming. Lancaster is bit slower than Porter so we can use it according to size and response time required. Snowball stemmer is a slightly improved version of the Porter stemmer and is usually preferred over the latter. It is not very clear which one will produce accurate results, so one has to experiment different methods and choose the one that gives better results. In this example, Porter Stemmer is used which is simple and speedy. Following code shows how to implement stemming on dataframe and new column stemmed_tokens is created: Output: Train data would be used to train the model and test data is the data on which the model would predict the classes and it will be compared with original labels to check the accuracy or other model test metrics. Try to balance the number of classes in both the sets so that the results are not biased or one of the reasons for insufficient model training. This is a crucial part of machine learning model. In real-world problems, there are cases of imbalanced classes which needs using techniques like oversampling minority class, undersampling majority class ( Resample function from scikit-learn packaged or generating synthetic samples using SMOTE functionality in Imblearn package . For this case, the data is split into two parts, train and test with 70% in train and 30% in test. While making the splitting, it is better to have equal distribution of classes in both train and test data. Here, function train_test_split from scikit-learn package is used. Output: As it can be seen from the above output, data is distributed for each classes proportionately. Number of rows for each sentiment in train and test are printed. Word embeddings are words mapped to real number vectors such that it can capture the semantic meaning of words. The methods tried in my previous posts of BOW and TFIDF do not capture the meaning between the words, they consider the words seperately as features. Word embeddings use some models to map a word into vectors such that similar words will be closer to each other. As shown in the below figure, for example some of the positive words which are adjectives will be closer to each other and vice versa for negative adjectives. It captures semantical and syntactical information of words. To train this model it takes into consideration the words surrounding that word of particular window size. There are different ways of deriving the word embedding vectors. Word2vec is one such method where neural embeddings model is used to learn that. It uses following two architectures to achieve this. Here the model predicts the word under consideration given context words within specific window. The hidden layer has the number of dimensions in which the current word needs to be represented at the output layer. Following diagram shows as example with window of size 2 for predicting vector for word ‘awesome’ given a sentence ‘Restaurant was awesome this time’. Skip gram is opposite of CBOW where it predicts embeddings for the surrounding context words in the specific window given a current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer. Following shows an example with window size of 2. In this case, we will be using gensim’s Word2Vec for creating the model. Some of the important parameters are as follows: Output: It is usually better to save the model in some file so you don’t have to rerun it every time doing the training for the classifier. In the next part, we will reload the model and see how to access the word2vec dictionary as well. Output: Next will see how to use the Word2Vec model to get the vector for documents in the dataset. Word2Vec vectors are generated for each review in train data by traversing through the X_train dataset. By simply using the model on each word of the review, we get the word embedding vectors for those words. We will be implementing average over all the vectors of words in a sentence and that will represent a sentence from our dataset. These vectors are stored in a csv file. You can directly create this in a dataframe but when there is a large amount of data it is better to write to a file as and when the vector is created and if the code breaks you can start from the point where it had broken. Following code, writes the vectors in the OUTPUT_FOLDER defined in the first step. Once the Word2Vec vectors are ready for training, we load it in dataframe. DecisionTreeClassifier is used here to do the sentiment classification. Decision tree classifier is Supervised Machine learning algorithm for classification. In this example, scikit-learn package is used for implementing the decision tree classifier class . The fit function is used to fit the input feature vectors against the sentiments in train data. Following code shows how to train the classifier with Word2Vec vectors. Output: This took ~36 seconds to train for our input data. clf_decision_word2vec variable can be now used to do the predictions. Time to see how the model worked out at the end of this all facade. Output: Classification Report shows the average accuracy which is 0.52 . This is a good result compared to the amount of data used for training. The predict function can be used on the model object to get the predicted class for the test data. Accuracy for positive and negative sentiments is better than neutral which makes sense as it is hard to distinguish the neutral comments compared to commonly used words in the positive and negative sentiment. This accuracy is little bit less compared to BOW Classification and TFIDF Classification done in previous posts. One thing to notice is that the total input dimension has reduced from vocab size of 30056 to 1000 in case of Word2Vec. That is here the dimension can be made custom of less size but as it is capturing the necessary things and only limited things to describe the words it is a good compromise between accuracy and computational complexity for the classification model. So now you can easily experiment with your own dataset with this method! I hope this helped you to understand how to use Word2Vec vectors to do the sentiment analysis on restaurant reviews data. Feel free to extend this code! This is applicable to any other text classification problems where multiple classes are there. If I can think about improving this model, I would use different hyper-parameters for decision classifier or even try out other classification models. Input parameters like size , min_count and window_size of word2vec function can be experimented to get a better accuracy than this. Instead of using average, min and max of word vectors of words in a sentence can be taken as well. Preprocessing can be changed to use lemmatization or other stemming algorithms to see how the results change. As always — Happy experimenting and learning :) -- -- 1 The Startup Big Data Consultant @Netlight | CoFounder @HuskyCodes | Web developer | Passionate about coding, dancing, reading Help Status About Careers Press Blog Privacy Terms Text to speech Teams\",\n",
        "\n",
        "\n",
        "    \"https://tasty.co/recipe/the-best-chewy-chocolate-chip-cookies\": \"Have a recipe of your own to share? Submit your recipe There are a few crucial steps to baking the chewiest, tastiest chocolate chip cookies. First, skip using chips and opt for chunks. Second, instead of using just one type of chocolate (like semisweet), use a mix of semisweet, milk, and dark chocolate. Third, take the time to allow the cookie dough to rest overnight in the refrigerator. Yes, we know it's a pain, but doing so will yield a cookie with a more complex flavor and delicious toffee notes. Lastly, use an ice cream scoop to portion the cookie dough onto the baking sheet. This will produce even-sized cookies every single time. Follow these simple steps, and you'll be rewarded with a cookie that's crisp and chewy on the outside and gooey on the inside! 1 hr 5 min 1 hr 5 min 20 minutes 20 min 15 minutes 15 min Inspired by buzzfeed.com 1 hr 5 min 1 hr 5 min 20 minutes 20 min 15 minutes 15 min for 12 cookies Estimated values based on one serving size. Build your cart with Tasty, then choose how you want to get your order from Walmart. I love this recipe so much!! I am NEVER using chocolate chips ever again! These cookies are decadent, chewy, and oh so delicious! I used light brown sugar and salted butter. I highly recommend this recipe to any chocolate chip cookie lover! if the cookies get hard, store them in and airtight bag or container with a slice of bread and they will become soft again soon I have made these about 4 times now! My tip is to not melt the butter until it’s completely liquid, then whisk it until the soft butter is incorporated with the fully melted butter before adding. Also if you let the dough rest in the fridge for 3 hours or more, they get nice and toffee-like. The cookies were so delicious 😋 They were a little bigger than expected. I put the cookies on a nonstick pan instead of parchment paper and they still turned out fine. 😍 They were really easy to make. I’m 12 😂 100% I’d make again! I bake them for 5 minutes less to avoid over cooking as they’re cooling. This ensures they will still be soft after even a week of storage (if they last that long). I’ve made this recipe twice now and I changed the recipe a tad bit. I added 1/4 cup more flour, 1/2 teaspoon of salt and I used 3/4 cup of nestle’s dark chocolate chips . I let them hang out in the fridge for about 2 hours. I also preheated my oven to 375 and let them bake for 10 mins and took them out to sit about 5 min to finish. Be sure to get the right kind of chocolate! Not chocolate chips. Get the chunks! Trust me, it makes a difference. When I was incorporating the melted butter and sugars, it doesn’t look the same like the one in the video. I continued the recipe but it ended up looking like cake batter, so I added 1/4 cup of flour and it turned out great! I recommend mixing chocolate chips and chocolate chunks for my diversity Use half melted butter and half soften butter that’s makes a chewier cookie Way too sweet, use less sugar These cookies are LEGIT the best cookies i’ve ever made! Have made them several times and they come out great every time. I use a bit less brown sugar and a bit more white sugar so the cookie will spread. Really good! Inspired by buzzfeed.com\",\n",
        "\n",
        "\n",
        "    \"https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57\": \"We can almost guarantee you won’t have leftovers—this is the best chocolate chip cookie recipe around! If you do end up with any extra chocolate chip cookies, they can easily be stored for later enjoyment. To keep your leftover cookies chewy and soft, store them at room temperature in resealable food-storage plastic bags or stacked in tightly covered containers. If you want to freeze your homemade chocolate chip cookies instead, wait until they’ve cooled completely. Stack them in between layers of wax paper in a covered freezer container or plastic freezer bag and freeze your cookies for up to 2 months. To thaw, keep them covered at room temperature for 1 to 2 hours. Go ahead and chow down! Want to switch it up? This chocolate chip cookie recipe can easily be customized with endless flavor combinations! Just replace the chocolate chips and nuts with the same amount of the new ingredients you want to add. Try adding macadamia nuts and white vanilla baking chips, or pack a double punch of peanut flavor with peanut butter chips and chopped peanuts. Make salted butterscotch-pecan cookies by swapping in butterscotch chips and chopped pecans, and then sprinkling your cookies with coarse salt before baking. We also have plenty of other chocolate chip cookie recipes to try. Can’t decide between cookies and brownies? Our Double Chocolate Chip Cookies , are packed full of decadent chocolate flavor. Our Oatmeal Chocolate Chip Cookies , offer next-level deliciousness, too—you really can never go wrong with a classic! Baking is a science, and we can prove it! Whether you prefer a soft, chewy chocolate chip cookie or a crisp, crunchy cookie, consider the factors that make a difference in cookie structure—from the ratio and type of sugars used to the amount of butter or shortening used, and how much flour is stirred in. If you like your cookies slightly crispy on the outside and chewy on the inside—what many people consider the perfect texture for a chocolate chip cookie—just follow this recipe exactly! Our Ultimate Chocolate Chip Cookies are truly the best chocolate chip cookies around—they’re called “ultimate” for a reason. Prefer your homemade chocolate chip cookies crispy and thin? Cut out the brown sugar completely, and increase the amount of granulated sugar to 1 1/2 cups. Granulated sugar contains less moisture and helps cookies spread as they bake. After taking your chocolate chip cookies out of the oven, let them cool on a cookie sheet for five minutes. The cookies will keep baking, allowing them to crisp up further as the moisture evaporates. If you want your chocolate chip cookies airy, soft and cakey, we’ve got you covered. Add in an extra egg for additional moisture and structure, and add in 2 tablespoons of milk with the egg to help soften the dough as it bakes and give the cookie a cake-like crumb. Increase the amount of flour to 3 cups to help the cookies rise (rather than spread). The perfectly cakey chocolate chip cookie is just a few steps away! Want to have a crowd-favorite cookie always at the ready? Follow this chocolate chip cookie recipe to get the perfect dough, and freeze it for later baking! Freeze individual unbaked dough balls on cookie sheets. Once the dough balls are frozen, place them in plastic freezer bags and freeze them for up to 2 months. When you’re ready to bake your cookies, take the frozen cookie dough balls from the freezer and place them on a cookie sheet. Let them sit at room temperature for 15 minutes as you preheat the oven. Keep an eye on your chocolate chip cookies as they bake—the bake time might need to be adjusted slightly. This hack guarantees delicious cookies in a snap!\",\n",
        "\n",
        "\n",
        "    \"https://www.geeksforgeeks.org/machine-learning/\": \"Machine Learning tutorial covers basic and advanced concepts, specially designed to cater to both students and experienced working professionals. This machine learning tutorial helps you gain a solid introduction to the fundamentals of machine learning and explore a wide range of techniques, including supervised, unsupervised, and reinforcement learning. Machine learning (ML) is a subdomain of artificial intelligence (AI) that focuses on developing systems that learn—or improve performance—based on the data they ingest. Artificial intelligence is a broad word that refers to systems or machines that resemble human intelligence. Machine learning and AI are frequently discussed together, and the terms are occasionally used interchangeably, although they do not signify the same thing. A crucial distinction is that, while all machine learning is AI, not all AI is machine learning. Machine Learning is the field of study that gives computers the capability to learn without being explicitly programmed. ML is one of the most exciting technologies that one would have ever come across. As it is evident from the name, it gives the computer that makes it more similar to humans: The ability to learn. Machine learning is actively being used today, perhaps in many more places than one would expect. Table of Content Answer : Machine learning develop programs that can access data and learn from it. Deep learning is the sub domain of the machine learning. Deep learning supports automatic extraction of features from the raw data. Answer : Answer : Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems. Answer :\",\n",
        "\n",
        "\n",
        "    \"https://builtin.com/machine-learning/machine-learning-basics\": \"Machine learning is an application of artificial intelligence where a machine learns from past experiences (input data) and makes future predictions. Machine learning is an application of artificial intelligence where a machine learns from past experiences (input data) and makes future predictions. It’s typically divided into three categories: supervised learning , unsupervised learning and reinforcement learning . This article introduces the basics of machine learning theory, laying down the common concepts and techniques involved. This post is intended for people starting with machine learning, making it easy to follow the core concepts and get comfortable with machine learning basics. Machine learning is an application of artificial intelligence in which a machine learns from past experiences or input data to make future predictions. There are three common categories of machine learning: supervised learning, unsupervised learning and reinforcement learning. In 1959, Arthur Samuel, a computer scientist who pioneered the study of artificial intelligence, described machine learning as “the study that gives computers the ability to learn without being explicitly programmed.” Alan Turing’s seminal paper introduced a benchmark standard for demonstrating machine intelligence, such that a machine has to be intelligent and responsive in a manner that cannot be differentiated from that of a human being. A more technical definition given by Tom M. Mitchell’s 1997 paper : “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” For example, a handwriting recognition learning problem: In order to perform the task T, the system learns from the data set provided. A data set is a collection of many examples. An example is a collection of features. More on Machine LearningAll Machine Learning Models Explained Machine learning is generally categorized into three types: Supervised learning, unsupervised learning, reinforcement learning. In supervised learning the machine experiences the examples along with the labels or targets for each example. The labels in the data help the algorithm to correlate the features. Two of the most common supervised machine learning tasks are classification and regression . In classification, the machine learning model takes in data and predicts the most likely category, class or label it belongs to based on its values. Some examples of classification include predicting stock prices and categorizing articles to news, politics or leisure based on its content. In regression, the machine predicts the value of a continuous response variable. Common examples include predicting sales of a new product or a salary for a job based on its description. When we have unclassified and unlabeled data, the system attempts to uncover patterns from the data . There is no label or target given for the examples. One common task is to group similar examples together called clustering . Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best. This is known as the reinforcement signal. For example, maximizing the points won in a game over a lot of moves. Regression is a technique used to predict the value of response (dependent) variables from one or more predictor (independent) variables. Most commonly used regressions techniques are linear regression and logistic regression . We will discuss the theory behind these two prominent techniques alongside explaining many other key concepts like gradient descent algorithm, overfit and underfit, error analysis, regularization, hyperparameters and cross validation techniques involved in machine learning. In linear regression problems, the goal is to predict a real-value variable y from a given pattern X . In the case of linear regression the output is a linear function of the input. Let ŷ be the output our model predicts: ŷ = WX+b Here X is a vector or features of an example, W are the weights or vector of parameters that determine how each feature affects the prediction, and b is a bias term. So, our task T is to predict y from X . Now ,we need to measure performance P to know how well the model performs. To calculate the performance of the model, we first calculate the error of each example i as: We then take the absolute value of the error to take into account both positive and negative values of error. Finally, we calculate the mean for all recorded absolute errors or the average sum of all absolute errors. Mean absolute error (MAE) equals the average of all absolute errors: A more popular way of measuring model performance is using Mean squared error (MSE) . This is the average of squared differences between prediction and actual observation. The mean is halved as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the half term. The main aim of training the machine learning algorithm is to adjust the weights W to reduce the MAE or MSE. To minimize the error, the model updates the model parameters W while experiencing the examples of the training set. These error calculations when plotted against the W is also called cost function J(w) , since it determines the cost/penalty of the model. So, minimizing the error is also called as minimizing the cost function J . In the gradient descent algorithm , we start with random model parameters and calculate the error for each learning iteration, keep updating the model parameters to move closer to the values that results in minimum cost. Repeat until minimum cost: In the above equation, we are updating the model parameters after each iteration. The second term of the equation calculates the slope or gradient of the curve at each iteration. The gradient of the cost function is calculated as a partial derivative of cost function J with respect to each model parameter wj , where j takes the value of number of features [1 to n] . α , alpha, is the learning rate, or how quickly we want to move towards the minimum. If α is too large, we can overshoot. If α is too small, it means small steps of learning, which increases the overall time it takes the model to observe all examples. There are three ways of doing gradient descent: In some problems the response variable isn’t normally distributed . For instance, a coin toss can result in two outcomes: heads or tails. The Bernoulli distribution describes the probability distribution of a random variable that can take the positive case with probability P or the negative case with probability 1-P . If the response variable represents a probability, it must be constrained to the range {0,1} . In logistic regression, the response variable describes the probability that the outcome is the positive case. If the response variable is equal to or exceeds a discrimination threshold, the positive class is predicted. Otherwise, the negative class is predicted. The response variable is modeled as a function of a linear combination of the input variables using the logistic function. Since our hypotheses ŷ has to satisfy 0 ≤ ŷ ≤ 1 , this can be accomplished by plugging logistic function or sigmoid function : The function g(z) maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification . Now, coming back to our logistic regression problem, let’s assume that z is a linear function of a single explanatory variable x . We can then express z as follows: And the logistic function can now be written as: g(x) is interpreted as the probability of the dependent variable. g(x) = 0.7 , gives us a probability of 70 percent that our output is one. Our probability that our prediction is zero is just the complement of our probability that it is one. For example, if the probability that it’s one is 70 percent, then the probability that it is zero is 30 percent. The input to the sigmoid function g doesn’t need to be a linear function. It can be a circle or any shape. We cannot use the same cost function that we used for linear regression because the sigmoid function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. In order to ensure the cost function is convex, and therefore, ensure convergence to the global minimum, the cost function is transformed using the logarithm of the sigmoid function. The cost function for logistic regression looks like: Which can be written as: So, the cost function for logistic regression is: Since the cost function is a convex function, we can run the gradient descent algorithm to find the minimum cost. We try to make the machine learning algorithm fit the input data by increasing or decreasing the model’s capacity. In linear regression problems, we increase or decrease the degree of the polynomials. Consider the problem of predicting y from x ∈ R . Since the data doesn’t lie in a straight line, the fit is not very good. To increase model capacity, we add another feature by adding the term x² to it. This produces a better fit. But if we keep on doing so x⁵ , fifth order polynomial), we may be able to better fit the data but it will not generalize well for new data. When the model has fewer features, it isn’t able to learn from the data very well. This means the model has a high bias. When the model has complex functions, it’s able to fit the data but is not able to generalize to predict new data. This model has high variance. There are three main options to address the issue of overfitting: Regularization can be applied to both linear and logistic regression by adding a penalty term to the error function in order to discourage the coefficients or weights from reaching large values. The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified linear regression error function: Where lambda is our regularization parameter. In order to minimize the error, we use the gradient descent algorithm. We keep updating the model parameters to move closer to the values that result in minimum cost. Then repeat until convergence, with regularization: With some manipulation, the above equation can also be represented as: The first term in the above equation will always be less than one: You can see it as reducing the value of the coefficient by some amount on every update. The cost function of the logistic regression with regularization is: Then, repeat until convergence with regularization: The regularization term used in the previous equations is called L2 , or ridge regularization. The L2 penalty aims to minimize the squared magnitude of the weights. There is another regularization called L1, or lasso: The L1 penalty aims to minimize the absolute value of the weights. Hyperparameters are higher-level parameters that describe structural information about a model that must be decided before fitting model parameters. Examples of hyperparameters we discussed so far include: Learning rate (alpha ) and regularization (lambda ). The process to select the optimal values of hyperparameters is called model selection . if we reuse the same test data set over and over again during model selection, it will become part of our training data, and the model will be more likely to over fit. The overall data set is divided into three categories: The training set is used to fit the different models, and the performance on the validation set is then used for the model selection. The advantage of keeping a test set that the model hasn’t seen before during the training and model selection steps is to avoid overfitting the model. The model is able to better generalize to unseen data. In many applications, however, the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. However, if the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this dilemma is to use cross-validation. More on Machine LearningUnderstanding Feature Importance in Machine Learning These are the steps for selecting hyper-parameters using K-fold cross-validation: Cross-validation allows us to tune hyperparameters with only our training set. This allows us to keep the test set as a truly unseen data set for selecting the final model. We’ve covered some of the key concepts in the field of machine learning, starting with the definition of machine learning and then covering different types of machine learning techniques. We discussed the theory behind the most common regression techniques (linear and logistic) alongside other key concepts of machine learning.\",\n",
        "\n",
        "\n",
        "    \"https://www.atlassian.com/blog/productivity/simple-ways-to-be-productive-at-work\": \"Atlassian Work Life is Atlassian’s flagship publication dedicated to unleashing the potential of every team through real-life advice, inspiring stories, and thoughtful perspectives from leaders around the world. Contributing Writer Work Futurist Senior Quantitative Researcher, People Insights Contributing Writer Principal Writer Subscribe Culture, tech, teams, and tips, delivered twice a month Subscribe Culture, tech, teams, and tips, delivered twice a month Subscribe Culture, tech, teams, and tips, delivered twice a month Subscribe Culture, tech, teams, and tips, delivered twice a month Our State of Teams 2024 report is live! Check it out here . Use these powerful strategies to get more done (while stressing less). Contributing Writer Get stories like this in your inbox You reach the end of your workday and glance down at your (now coffee-stained) to-do list. You’re immediately overcome with frustration as you realize that barely half the tasks are checked off. Seriously, what happened? You’ve been at your desk for the better part of eight hours. Why didn’t you get more accomplished? Maintainin a high level of productivity is not intuitive or easy – but you don’t have to resign yourself to feeling discouraged and depleted at the end of every workday. We rounded up ten top-notch, rock-solid tips you can put into play to channel your focus, and defeat your to-do list – plus a quiz to help get you started. “People naturally have ebbs and flows in their work processes or in how well they can focus,” explains Dr. Melissa Gratias, a workplace productivity coach and speaker. These peaks and valleys in your focus and motivation are naturally occurring in your body, driven by your ultradian rhythms . You can’t compete with science. So rather than doubling down on your caffeine intake, the smarter move is to pay close attention to the times of day when you feel most energized. Keep a journal for at least a week or two (one day isn’t long enough to identify trends) and note how you feel. You’ll have an easier time spotting your biological prime time – the times you’re most “in the zone.” With that information, you can allocate your work more effectively . Plan your deep, complex, or creative work for your golden hours (you can even block off your calendar) and save menial tasks for the times when you feel a little more drained. “The critical behavior that I advise people not to fall into is setting yourself up for failure before the day even begins,” Gratias says. “If we pull out a task list of 25 things to do, we’re guaranteed that we’re going to be disappointed in our progress at the end of the day.” Try whittling down to your priorities – Gratias recommends choosing between five and nine tasks you want to make progress on that day. This ties back to a psychological principle called “The Magical Number Seven, Plus or Minus Two.” Other people swear by the similar 1-3-5 rule for an empowering to-do list. Pick one big thing you need to accomplish that day, three medium things, and five little things. If you’re really struggling to figure out what deserves some real estate on your list, use an Eisenhower Matrix (sometimes called a prioritization matrix) to sort through your tasks and determine which ones deserve top billing – and which ones can be delegated or fall off your list entirely. “Distractions and interruptions are for sure an impediment to productivity,” says Gratias. She explains that these distractions fall into two different categories: And it’s not only the distraction itself that robs you of your time – it’s also the time and energy you have to spend refocusing (which research has shown can take upwards of 23 minutes). While Gratias cautions that you’ll never completely eliminate distractions, you can reduce them by: “‘I am a great multitasker!’ That’s my favorite misconception that I hear from employees,” explains Dr. Larry Rosen, Professor Emeritus and former Chair of Psychology at California State University Dominguez Hills. Here’s the thing: Research shows that the human brain is actually incapable of multitasking. Instead, you’re rapidly switching between different tasks – appropriately referred to as “task switching” or “ context switching .” You’re basically interrupting yourself, and you know now that those self-imposed disruptions only tank your productivity. So how do you get your brain to do one thing at once? Say out loud the one thing you’re going to work on (for example, “I’m going to finish this slide deck.”). And that’s it! It’s called external self-talk , and plenty of research has shown that it can have a real impact on your behavior. Even if you’re technically focusing on only one task at a time, repeatedly switching between different types of work – you answer an email then update a report then work on your slide deck then answer another email – can be mentally draining. You use different parts of your brain for different tasks, which means you’re majorly straining your noggin by not having any sort of systematic approach to your work. Try batching your tasks, which is essentially grouping similar tasks together and doing them all at once. You can even try time blocking , where you set specific time windows for certain types of tasks (for example, you’ll answer emails from 9am to 10am). You might still need to occasionally handle things outside of their designated time slots, but any effort to stick related tasks together will give your brain some welcomed respite. It’s not just the stuff you do during the workday that impacts your productivity—the things you do outside of work carry a lot of weight too. Maintaining healthy habits is a lot easier said than done, but even seemingly small changes can have a big impact on your energy levels, focus, and overall mood. Here are a few quick things to try: It’s tempting to think that more time at your desk means you’ll get more done. But in reality, studies show that taking regular breaks can can actually boost your productivity. If you’re prone to getting sucked into your work and forgetting to step away, using a time management method like the Pomodoro Technique will ensure you get up for a five-minute break between every 25-minute work period. Want to maximize your impact in that short time away? Head outside for some fresh air. Studies show that getting out into nature can alleviate mental fatigue. That majority of us who suddenly found themselves working from home in Spring 2020 (shudder) know how less-than-ideal surroundings can affect our productivity. That’s why it’s well worth curating a workspace that helps you feel your most focused and motivated. That can mean something different to everybody, but here are a few suggestions: Other efforts, like relying on a password manager, creating templates, and using a centralized project management platform mean you can spend less time searching for what you need and more time focused on your actual work. When your environment can have such a big impact on your focus and productivity, it’s worth trying to switch it up every now and then too. Move from your desk to answer some emails on your couch. Or bring your laptop out to your patio. Or do a few hours of work from your favorite coffee shop . Not only does this build in an extra break (you have to pause what you’re doing and relocate), but it also helps you buckle down. Your brain loves novelty and releases dopamine when it’s presented with something new and exciting. Dopamine isn’t just a “feel good” brain chemical – it’s a powerful motivator too. “I think one of the most insidious beliefs that limits productivity more than anything else is perfectionism,” explains Gratias. This perfectionism not only leads to failure to start things because “if we can’t finish them perfectly, we don’t even begin,” but it also eventually leads to failure to finish because “if it’s not perfect, we keep working on it and keep tweaking it.” While this desire to do spotless work is admirable, it can also significantly hinder your progress and productivity. How can you stop obsessing and focus on progress over perfection? When you have a task or a project, set a timebox (i.e. a certain span of time, such as 15 minutes or an hour) that dictates how long you’ll work on that specific item. When the timebox ends, that task is as done as it’s going to get for now. Not only does this tactic instill a sense of urgency (which inspires you to get moving), but it also removes some pressure – you’re more focused on seeing how much progress you can make in that time period, rather than working until the entire task is completed. Most of us have a lot to do. But even with high expectations and the best intentions, it’s hard to muster the motivation and make the most of your time at work. The truth is that you’re human – you won’t operate at peak efficiency all day every day. But there are still plenty of things you can try to boost your productivity levels and transform that end-of-day glance at your (reasonable) to-do list from disheartening to gratifying. Get stories like this in your inbox Kat Boogaard Contributing Writer How you work is just as important as the work you're doing. Laying the groundwork for better employee health and happiness. Embrace transparency, foster a sense of belonging, form connections – and have fun along the way. How you work is just as important as the work you're doing. Laying the groundwork for better employee health and happiness. Embrace transparency, foster a sense of belonging, form connections – and have fun along the way. How you work is just as important as the work you're doing. Laying the groundwork for better employee health and happiness. Embrace transparency, foster a sense of belonging, form connections – and have fun along the way. By Atlassian Culture, tech, teams, and tips, delivered twice a month These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information. These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising. These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly. These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\",\n",
        "\n",
        "    \"https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/\": \"Embeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search! At a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs. When calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings. There are many embedding models to pick from. By default, LlamaIndex uses text-embedding-ada-002 from OpenAI. We also support any embedding model offered by Langchain here , as well as providing an easy to extend base class for implementing your own embeddings. Most commonly in LlamaIndex, embedding models will be specified in the Settings object, and then used in a vector index. The embedding model will be used to embed the documents used during index construction, as well as embedding any queries you make using the query engine later on. You can also specify embedding models per-index. If you don't already have your embeddings installed: Then: To save costs, you may want to use a local model. This will use a well-performing and fast default from Hugging Face. You can find more usage details and available customization options below. The most common usage for an embedding model will be setting it in the global Settings object, and then using it to construct an index and query. The input documents will be broken into nodes, and the embedding model will generate an embedding for each node. By default, LlamaIndex will use text-embedding-ada-002 , which is what the example below manually sets up for you. Then, at query time, the embedding model will be used again to embed the query text. By default, embeddings requests are sent to OpenAI in batches of 10. For some users, this may (rarely) incur a rate limit. For other users embedding many documents, this batch size may be too small. The easiest way to use a local model is: LlamaIndex also supports creating and using ONNX embeddings using the Optimum library from HuggingFace. Simple create and save the ONNX embeddings, and use them. Some prerequisites: Creation with specifying the model and output path: And then usage: We also support any embeddings offered by Langchain here . The example below loads a model from Hugging Face, using Langchain's embedding class. If you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own! The example below uses Instructor Embeddings ( install/setup details here ), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \\\"instructions\\\" on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic. You can also use embeddings as a standalone module for your project, existing application, or general testing and exploration. We support integrations with OpenAI, Azure, and anything LangChain offers.\",\n",
        "\n",
        "    \"https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/\": \"Alongside the rise of Large Language Models (LLM), has risen a swath of programming frameworks that aim to make it easier to build applications on top of them, such as LangChain and LlamaIndex . LlamaIndex is a programming framework that aims to simplify the development of LLM-enabled applications that leverage Retrieval Augmented Generation (RAG). A core conceptual component to understanding the mechanisms behind LlamaIndex is that of query Response Modes . LlamaIndex has 5 built-in Response Modes: compact, refine, tree_summarize, accumulation, and simple_summarize. In this article, I will: To evaluate each mode, I put them to the test in helping answer a question that has plagued the world since that fateful day in November 1963: Who shot President John F. Kennedy? I will demonstrate how each Response Mode works by asking LlamaIndex to summarize the conclusions of the ‘ Warren Commission Report ’, the 250,000-word Congressional report that investigated the assassination of President Kennedy. This document far exceeds the token limit for GPT-4 and most other mainstream LLMs and requires the use of RAG techniques for an LLM to answer questions on it using only passed in contextual data (not training data). The concept of Retrieval Augmented Generation (RAG) describes an approach to allow an LLM to answer questions based on data that it wasn’t originally trained on. In order to do this, an LLM must be fed this data as part of the prompt, which is generally referred to as ‘context’. However, LLMs have limited input size restrictions (also called token input size), which make it impossible and impractical to pass large datasets, such as the Warren Report, in their entirety via the prompt. Instead, with the RAG-approach, a query to an LLM is broken into two parts: a ‘retrieval step’ and then a ‘generation step’. The ‘retrieval step’ attempts to identify the portions of the original dataset that are most relevant to a user supplied query and to only pass this subset of data to an LLM, alongside the original query, as part of the ‘generative step’. Essentially, RAG is a mechanism to work within the input size restrictions of LLMs by only including in the prompt the most relevant parts of the dataset needed to answer a query. Usually, the ‘retrieval’ portion of RAG utilizes tried-and-true semantic search algorithms such as Cosine Similarity , alongside a Vector Databases to perform this step. For the purposes of this article and the testing of the Response Modes, the question I am seeking to get an answer to is: “What are the conclusions of the Warren Report?” For the purposes of this article, in the retrieval step I set my code to return the top 5 most relevant ‘chunks’ of data that relate to my original query from the Warren Report as part of the Cosine similarity algorithm. These 5 chunks of data are then passed forward to the LLM, which is where LlamaIndex Response Modes come into play. When building an LLM enabled application that utilizes RAG techniques, it is still likely that the subset of data returned as part of the retrieval step, which are normally referred to as ‘chunks’, will still be too large to fit within the input token limit for a single LLM call. Most likely, multiple calls will need to be made to the LLM in order to derive a single answer that utilizes all of the retrieved chunks. LlamaIndex Response Modes govern the various approaches that can be used to break down, sequence and combine the results of multiple LLM calls with these chunks to return a single answer to the original query. As of writing, there are 5 basic Response Modes that can be used when querying with LlamaIndex. In evaluating the 5 different Response Modes, I utilize the following frameworks and tools: For each test, I pose the same question: “What are the main conclusions of the Warren Report regarding the assassination of President Kennedy?” The following is the Python code I used to evaluate each Response Mode, this one configured to use the compact Response Mode: Note also that in my call to initialize the query_engine in LlamaIndex, I set the similarity_top_k parameter to 5, which tells LlamaIndex to return the top 5 chunks of data that are semantically similar to the query as part of the retrieval step. For all Response Modes, the same 5 chunks of text are returned from the Warren Commission Report, which are available for you to view in the table below: The Response Mode called compact is the default mode used by LlamaIndex if none is specified. The way the compact mode works is that for each chunk that is returned from the retrieval step, LlamaIndex concatenates as many of those chunks together into the largest possible string that fits into a single prompt to the GPT-4 LLM. In our example, the first 4 chunks of matched text fit into the context window for a single GPT-4 call, which means that it requires 2 LLM calls to answer our query on the conclusion of the Warren Report. The 1st call made using the compact Response Mode always uses the text_qa_template prompt: LlamaIndex then takes the answer returned to this prompt, the next concatenated set of chunks (in our case simply Chunk 5) and passes it to the LLM along with the last few sentences of Chunk 4 using the refine_template prompt: As you can see, the compact Response Mode doesn’t answer the question at anything resembling a coherent answer. In fact, the LLM ends up throwing up its hands and doing a spot on rendition of a high school student fumbling their way trying to answer a question they simply have no clue about. The likely reason why the compact Response Mode was unable to answer the question is that the first 4 chunks of text from the Warren Report don’t actually contain the conclusions of the report, which only appears in Chunk 5 (which oddly has a lower Cosine similarity score than the proceeding chunks). Thus, the structure of the refine_template is such that if the first set of calls has gone down the wrong path, it’s difficult for the final prompt to steer the LLM back onto the right track. You can see the full-text log for both of the LLM calls made with the compact Response Mode below: The second Response Mode that LlamaIndex provides is refine . The refine mode is very similar to the compact Response Mode, except that instead of attempting to concatenate as many chunks as it can to maximize the use of the LLM token limit, LlamaIndex only include 1 chunk of retrieved data for each LLM call. Starting with the text_qa_template , LlamaIndex passes in Chunk 1 to the LLM. After that, LlamaIndex then progresses sequentially through each of the remaining chunks one at a time; with each subsequent LLM call using the refine_template to build upon the answer returned from the answer returned from the previous chunk. While the compact Response Mode tried to obfuscate it’s inability to answer the question behind a wall of hand-waving text, the refine Response Mode yields a much more succinct, yet equally useless answer. Much like the compact mode, the refine is much more likely to not be able to properly answer the question when the initial chunks passed to it are less relevant. In our case, the first chunk of data from the Warren Report is the introduction and forward parts of the report which do not contain any conclusions whatsoever, which is likely why it was never able to return a proper answer to our query. You can see the full-text log for all of the LLM calls made with the refine Response Mode below: The third, and by far most effective, Response Mode is tree_summarize. The astute reader might surmise from the use of the word ‘tree’ that there is a recursive property to this response mode, and they would be correct. The tree_summarize mode in its base case makes a series of LLM calls that concatenate chunks of retrieved data so that it maximizes the input token limit for the LLM. It then takes the outputs of each of these base case responses, and then passes them together to the LLM and instructs it to derive an answer using those initial answers as context. For anyone familiar working with LangChain, the tree_summarize Response Mode is essentially the same as LangChain’s MapReduceDocumentChain . In our analysis of the Warren Report, the tree_summarize mode required 3 calls to the LLM, 2 for processing the 5 chunks of data, and then 1 for combining the answers from the first 2. The prompt template used by the tree_summarize Response Mode is quite simple and the same each time, regardless if we are working at the ‘leaf’ level or higher up in the processing tree: After making a similar call that includes only Chunk 5, tree_summarize then uses the following prompt template to combine the answers: The tree_summarize response mode nails the answer and delivers a thoughtful and complete summary of the findings of the Warren Report that correctly identify Lee Harvey Oswald as the assassin, while also in the same breath, disabusing any notion of a conspiracy or second shooter. Whereas the refine and compact modes were led astray by the irrelevance of the first set of chunks they analyzed, tree_summarize mode overcomes this as it is uses a map-reduce pattern to have the LLM independently analyze each concatenated chunk of data and then in a separate prompt combine the outputs of those into a single answer. You can see the full-text log for all of the LLM calls made using the tree_summarize Response Mode below: The accumulate Response Mode is quite simple, LlamaIndex makes 1 call per retrieved chunk and then returns every ‘non-null’ answer together as an array of answers. For each of the calls that LlamaIndex makes in accumulate mode, it uses a similar template to tree_summarize : The returned result from LlamaIndex is an array of strings of length 2, containing the responses returned from the LLM for chunks 4 and 5. The answers for chunks 1,2,3 are not included in this result, because for each of those calls, the LLM returned the logical equivalent of a ‘null’ response in the form of 'The context does not provide information on the main conclusions of the Warren Report regarding the assassination of President Kenne dy.’ The accumulate mode doesn’t so much answer the question, but instead returns n answers to the question with each of the answers being scoped simply to the context chunk passed to the LLM in that call. It is the responsibility then of the calling application to take the list of answers to produce an actual final answer to the query. ‘Accumulate’ doesn’t work well for something like the Warren Report where each chunk doesn’t necessarily contain the complete information to answer the question. You can see the full-text log for all of the LLM calls made using the accumulate Response Mode below: The final LlamaIndex Response Mode is simple_summarize . This perhaps the most basic and straightforward of the response modes. In this mode, LlamaIndex truncates all text chunks so that all chunks can be concatenated and passed into the LLM in a single call. No matter how many chunks are retrieved, there will only ever be a single call made to the LLM. In our example, the prompt template that is used in simple_summarize mode looks like: Surprisingly, the answer provided by simple_summarize is not bad and almost as complete as the one provided by tree_summarize However, I would attribute this more to luck then a structural advantage of the mechanism. With the simple_summarize mode, as the number of chunks returned from the retrieval step goes up, its likely that the ultimate answer returned will decrease in quality due to the knowledge being lost in the truncated segments of each chunk. You can view the full-text log of the prompt made in the simple_summarize test here . Looking at the results of my tests, the tree_summarize Response Mode returned the most comprehensive and complete answer to the question “What are the conclusions of the Warren Report?”. The recursive, map-reduce like algorithm it employs allows it to build up the correct answer looking at all matched chunks of text from the Warren Commission Report and the responses from GPT-4 to each of them. This is not to say that tree_summarize is the only Response Mode you need when building a RAG-style application, however for the purposes of summarizing content from across a large body of text, it clearly has structural advantages that lend itself more effective than the other modes. However, it’s important to understand the different motivations behind each of the other 4 Response Modes and to know which circumstances each might be the best tool for the job. With the rapidly increasing sizes of input token limits for newer LLM models, it can be argued that the need for RAG will slowly diminish. However, even in a world where LLMs are able to accept million token inputs, it will always behoove a consumer of an LLM to maximize the efficiency of the each token used in the context passed in each LLM call. As such, the LlamaIndex framework provides a very neat and consistent programming model to build RAG-style applications, certainly more so than LangChain. For those of you interested in learning more about LlamaIndex and building with it, I encourage you to visit the LlamaIndex documentation , as the team has done an excellent job of building a set of , clear and easy to work through tutorials that outline the many capabilities of LlamaIndex. Bobby Gill is the co-founder and Chief Architect of BlueLabel, an award winning digital product agency headquartered in New York. With over two decades of experience in software development, he is a seasoned full-stack engineer, software architect, and AI practitioner. Bobby currently leads the BlueLabel AI/ML practice, where he is leading a team of engineers operationalizing the transformational capabilities of generative AI within BlueLabel and for a number of enterprise clients. A Digital Transformation Agency. 4.8/5 Overall Rating © 2024 BlueLabel | All Rights Reserved – Total Rating 4.8 out of 5 based on 40+ reviews\"\n",
        "\n",
        "\n",
        "}\n",
        "documents = [Document(text=content, metadata={\"url\": url}, excluded_embed_metadata_keys = [\"urls\"]) for url, content in fake_dataset.items()]"
      ],
      "metadata": {
        "id": "CJOpb1imhWxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
        "sentence_window_node_parser = SentenceWindowNodeParser.from_defaults(\n",
        "        window_size=3,\n",
        "        window_metadata_key=\"window\",\n",
        "        original_text_metadata_key=\"original_text\",)\n",
        "\n",
        "sentence_window_node_parser_nodes = sentence_window_node_parser.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "J53frF3IULI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64rdZUp8vXV_",
        "outputId": "c9a3f736-9678-4161-836d-59dcd845afcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "z-nu4ggxvfQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " can try num_questions_per_chunk="
      ],
      "metadata": {
        "id": "Ft9FdLN2uQgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_generator = DatasetGenerator.from_documents(documents=documents)\n",
        "eval_questions = data_generator.generate_questions_from_nodes()\n",
        "eval_questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW6MFTpnvHcF",
        "outputId": "e09f4ab2-4e3f-4b2e-8a03-eddbfef9dcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/llama_index/core/evaluation/dataset_generation.py:215: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
            "  return cls(\n",
            "/usr/local/lib/python3.10/dist-packages/llama_index/core/evaluation/dataset_generation.py:312: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
            "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is reinforcement learning (RL) and how does it mimic the trial-and-error learning process used by humans?',\n",
              " 'How do RL algorithms use a reward-and-punishment paradigm to process data?',\n",
              " 'What is one benefit of using RL algorithms in complex environments with many rules and dependencies?',\n",
              " 'How do RL algorithms differ from traditional machine learning algorithms in terms of data labeling?',\n",
              " \"Why is RL particularly well-suited for scenarios where actions have prolonged consequences and feedback isn't immediately available?\",\n",
              " 'Give an example of how RL can be applied in real-world use cases, such as recommendation systems.',\n",
              " 'How does RL differ from traditional optimization methods in solving problems?',\n",
              " 'In what type of system does RL adjust to fluctuating resource needs and choose optimal instance types, quantities, and configurations?',\n",
              " 'How do RL agents generalize their learned strategies across similar but not identical tasks?',\n",
              " 'What is the overall goal of using reinforcement learning in artificial intelligence systems?',\n",
              " 'How can reinforcement learning be used to optimize long-term energy efficiency and cost?',\n",
              " 'Give an example of how reinforcement learning can be applied in recommendation systems.',\n",
              " 'Explain the concept of the exploration-exploitation trade-off in reinforcement learning.',\n",
              " 'What are some key algorithms used in reinforcement learning, and how do they differ from each other?',\n",
              " 'Describe the difference between model-based and model-free reinforcement learning.',\n",
              " 'How does a model-based reinforcement learning agent build an internal representation of the environment?',\n",
              " 'Provide an example of how a model-free reinforcement learning approach can be used in a real-world scenario.',\n",
              " 'How does reinforcement learning mimic the learning process in animal and human reinforcement learning in behavioral psychology?',\n",
              " 'What is the Markov decision process, and how is it used in reinforcement learning?',\n",
              " 'Explain how deep reinforcement learning applies deep neural networks to optimize learning strategies.',\n",
              " 'What is model-free reinforcement learning and when is it best to use?',\n",
              " 'How does a self-driving car learn to navigate city traffic using reinforcement learning?',\n",
              " 'What are the distinctions between supervised learning, unsupervised learning, and reinforcement learning?',\n",
              " 'Explain the difference between supervised learning and reinforcement learning in terms of input-output mapping.',\n",
              " 'How do unsupervised learning algorithms find patterns and relationships within data?',\n",
              " 'Why may it be challenging to deploy reinforcement learning algorithms in real-world environments?',\n",
              " 'How can Amazon Web Services (AWS) help developers and data scientists develop and deploy reinforcement learning algorithms?',\n",
              " 'What offerings does AWS provide for developing and testing reinforcement learning models?',\n",
              " 'How can developers use AWS RoboMaker to run and automate simulations with reinforcement learning algorithms for robotics?',\n",
              " 'What hands-on experience can developers gain with AWS DeepRacer in training reinforcement learning models?',\n",
              " 'What is the main focus of the AI-Generated Text Detection project described in the document?',\n",
              " 'How does the project use BERT to identify AI-generated text?',\n",
              " 'What are the steps involved in the comprehensive approach to AI-generated text detection outlined in the document?',\n",
              " 'How does the project handle data preprocessing to enhance the accuracy of the model?',\n",
              " 'What role does BERT tokenization play in preparing text for input into the detection model?',\n",
              " 'Can you explain the process of model training in the context of distinguishing between genuine and AI-generated text?',\n",
              " 'How are predictions generated by the trained model and what do they highlight?',\n",
              " 'In what format are the results of the detection process saved for user review and analysis?',\n",
              " 'How does the document encourage contributions from the community to enhance the project?',\n",
              " 'What is the significance of following the project and connecting with the author for more updates and projects?',\n",
              " 'What is the purpose of using Word2Vec for sentiment classification on Yelp restaurant review text data?',\n",
              " 'How is the Yelp restaurant review dataset formatted and loaded into a pandas dataframe?',\n",
              " 'Explain the process of deriving sentiment indication from the stars column in the dataset.',\n",
              " 'Why is preprocessing necessary for text data before performing sentiment analysis?',\n",
              " 'What are stop words and why are they commonly removed in NLP tasks?',\n",
              " 'How does tokenization help in transforming words to numbers for sentiment analysis?',\n",
              " 'Why is the problem of imbalanced classes not addressed in the post on sentiment classification using Word2Vec?',\n",
              " 'What are some common techniques used in preprocessing text data for sentiment analysis?',\n",
              " 'How does the removal of stop words affect the sentiment analysis process?',\n",
              " 'What are some potential challenges or limitations of using Word2Vec for sentiment classification?',\n",
              " 'Explain the importance of not removing stop words in sentiment analysis and provide an example from the text.',\n",
              " 'Describe the process of tokenization and its significance in natural language processing.',\n",
              " 'Compare and contrast stemming and lemmatization in text preprocessing.',\n",
              " 'How can imbalanced classes in machine learning datasets affect model training, and what techniques can be used to address this issue?',\n",
              " 'Discuss the concept of word embeddings and how they differ from traditional methods like BOW and TFIDF.',\n",
              " 'Explain the Word2vec model and its use in generating word embedding vectors.',\n",
              " \"How does Gensim's simple_preprocess function aid in text preprocessing?\",\n",
              " 'What is the purpose of splitting data into train and test sets in machine learning, and how is it done in the text?',\n",
              " 'Why is it important to balance the number of classes in both the train and test datasets?',\n",
              " 'How does the Porter Stemmer algorithm work, and how is it implemented in the text for stemming tokens in a dataframe?',\n",
              " 'Explain the difference between word embeddings and traditional methods like BOW and TFIDF for sentiment classification.',\n",
              " 'How does Word2vec capture the semantic and syntactical information of words?',\n",
              " 'Describe the two architectures used in Word2vec for generating word embeddings.',\n",
              " 'What is the difference between Skip gram and CBOW in Word2vec?',\n",
              " 'How are Word2vec vectors generated for each review in the training data?',\n",
              " 'Why is it important to save the Word2vec model in a file during training for sentiment classification?',\n",
              " 'How is the DecisionTreeClassifier used for sentiment classification in this example?',\n",
              " 'What is the average accuracy reported in the Classification Report for the sentiment classification using Word2vec?',\n",
              " 'Why is the input dimension reduced from the vocabulary size to 1000 in the Word2vec model?',\n",
              " 'How can Word2vec vectors be used for sentiment analysis on restaurant reviews data?',\n",
              " 'What is the reason for the higher accuracy in classifying positive and negative sentiments compared to neutral sentiments in sentiment analysis using Word2Vec?',\n",
              " 'How does the input dimension change when using Word2Vec compared to Bag of Words (BOW) and TFIDF classification methods?',\n",
              " 'How can the dimension size be customized in Word2Vec for sentiment analysis?',\n",
              " 'What is the advantage of using Word2Vec for sentiment analysis in terms of accuracy and computational complexity?',\n",
              " 'How can one experiment with different hyperparameters in the Word2Vec classification model to improve accuracy?',\n",
              " 'What are some potential improvements that can be made to the Word2Vec sentiment analysis model?',\n",
              " 'How can different classification models be used in conjunction with Word2Vec for text classification problems?',\n",
              " 'What are some alternative preprocessing techniques that can be used in Word2Vec for sentiment analysis?',\n",
              " 'How can the average, minimum, and maximum word vectors of words in a sentence be utilized in Word2Vec for sentiment analysis?',\n",
              " 'How can lemmatization or other stemming algorithms impact the results of sentiment analysis using Word2Vec?',\n",
              " 'What are the key steps to baking the chewiest, tastiest chocolate chip cookies according to the context?',\n",
              " 'Why is it recommended to use a mix of semisweet, milk, and dark chocolate instead of just one type of chocolate in the cookie recipe?',\n",
              " 'How long is it suggested to allow the cookie dough to rest in the refrigerator for optimal flavor?',\n",
              " 'What is the recommended tool for portioning the cookie dough onto the baking sheet?',\n",
              " 'How can you ensure that cookies remain soft if they become hard over time?',\n",
              " 'What adjustments did one person make to the recipe to achieve a chewier cookie?',\n",
              " 'How did someone modify the baking temperature and time in their cookie recipe?',\n",
              " 'What advice was given regarding the type of chocolate to use in the cookie recipe?',\n",
              " 'How did one person adjust the cookie dough consistency when it appeared too thin?',\n",
              " 'What is the recommended combination of sugars to use in the cookie recipe for optimal spreading?',\n",
              " 'How should leftover chocolate chip cookies be stored to keep them chewy and soft?',\n",
              " 'What are some suggested flavor combinations to customize the chocolate chip cookie recipe?',\n",
              " 'What is the key factor in determining whether a chocolate chip cookie will be crispy or chewy?',\n",
              " 'How can you make your homemade chocolate chip cookies crispy and thin?',\n",
              " 'What adjustments should be made to the recipe to achieve a cakey chocolate chip cookie?',\n",
              " 'How can you prepare the chocolate chip cookie dough for later baking?',\n",
              " 'What is the recommended method for thawing frozen chocolate chip cookie dough balls?',\n",
              " 'What is the suggested bake time adjustment when using frozen cookie dough balls?',\n",
              " 'How can you ensure that your chocolate chip cookies have the perfect texture of being crispy on the outside and chewy on the inside?',\n",
              " 'What is the benefit of adding an extra egg and milk to the chocolate chip cookie dough for a cake-like crumb?',\n",
              " 'What is the relationship between machine learning and artificial intelligence?',\n",
              " 'Define machine learning and explain how it differs from traditional programming.',\n",
              " 'How does machine learning help computers learn without being explicitly programmed?',\n",
              " 'What are the different types of machine learning techniques mentioned in the tutorial?',\n",
              " 'How does deep learning differ from traditional machine learning?',\n",
              " 'How is historical data used in machine learning algorithms to make decisions?',\n",
              " 'Can you explain the concept of supervised learning in machine learning?',\n",
              " 'How does unsupervised learning differ from supervised learning in machine learning?',\n",
              " 'Why is machine learning considered one of the most exciting technologies today?',\n",
              " 'How is reinforcement learning used in machine learning systems?',\n",
              " 'Define machine learning and explain the three categories it is typically divided into.',\n",
              " 'Who is Arthur Samuel and what did he contribute to the study of artificial intelligence?',\n",
              " 'Explain the difference between supervised learning, unsupervised learning, and reinforcement learning.',\n",
              " 'What is the goal of regression in machine learning and what are the common regression techniques used?',\n",
              " 'How is performance measured in machine learning models and what are some common performance metrics used?',\n",
              " 'Describe the concept of clustering in machine learning and provide an example of when it might be used.',\n",
              " 'What is reinforcement learning and how does it differ from supervised and unsupervised learning?',\n",
              " 'Explain the concept of mean squared error (MSE) and how it is used to measure model performance.',\n",
              " 'How does linear regression work in machine learning and what are the key components involved in the prediction process?',\n",
              " 'Discuss the importance of regularization in machine learning and how it helps prevent overfitting.',\n",
              " 'What is the main task in machine learning discussed in the context?',\n",
              " 'How is performance measured in machine learning models?',\n",
              " 'Explain the difference between Mean Absolute Error (MAE) and Mean Squared Error (MSE).',\n",
              " 'What is the purpose of adjusting the weights in a machine learning algorithm?',\n",
              " 'Describe the process of gradient descent in machine learning.',\n",
              " 'How is logistic regression different from linear regression?',\n",
              " 'What is the significance of the sigmoid function in logistic regression?',\n",
              " 'Why is it important to transform the cost function in logistic regression?',\n",
              " 'How does the cost function for logistic regression differ from linear regression?',\n",
              " \"How does the machine learning algorithm fit the input data by adjusting the model's capacity in linear regression problems?\",\n",
              " 'Explain why the cost function for logistic regression is transformed using the logarithm of the sigmoid function.',\n",
              " \"How does increasing the model's capacity in logistic regression relate to adding features like x² or x⁵?\",\n",
              " 'What is the difference between high bias and high variance in a machine learning model?',\n",
              " 'How does regularization help address the issue of overfitting in linear and logistic regression?',\n",
              " 'Describe the difference between L2 (ridge) regularization and L1 (lasso) regularization.',\n",
              " 'What are hyperparameters in machine learning and why are they important in model selection?',\n",
              " \"Why is it important to keep a test set that the model hasn't seen before during training and model selection?\",\n",
              " 'How does cross-validation help in tuning hyperparameters using the training set?',\n",
              " 'What are the advantages of using cross-validation in machine learning model development?',\n",
              " 'Can you explain the process of selecting hyperparameters using K-fold cross-validation in machine learning?',\n",
              " 'What are some strategies mentioned in the article to increase productivity at work?',\n",
              " 'How can identifying your biological prime time help improve productivity?',\n",
              " 'What is the \"Magical Number Seven, Plus or Minus Two\" principle and how does it relate to productivity?',\n",
              " 'How can an Eisenhower Matrix help in prioritizing tasks for the day?',\n",
              " 'Why is multitasking not an effective way to work, according to the article?',\n",
              " 'How can distractions and interruptions impact productivity at work?',\n",
              " 'What is external self-talk and how can it help improve focus on tasks?',\n",
              " 'According to Dr. Melissa Gratias, what is the downside of setting a task list with too many items?',\n",
              " 'How can keeping a journal help in identifying peak energy times for work?',\n",
              " 'What is the significance of allocating deep, complex, or creative work to your most energized hours?',\n",
              " 'What is the term used to describe the act of rapidly switching between different tasks, as opposed to multitasking?',\n",
              " 'How can external self-talk help improve productivity according to the context information?',\n",
              " 'What is the benefit of batching tasks and time blocking in terms of productivity?',\n",
              " 'How can taking regular breaks, such as using the Pomodoro Technique, boost productivity?',\n",
              " 'Why is it recommended to curate a workspace that helps you feel focused and motivated?',\n",
              " 'How can relying on a password manager and creating templates help improve productivity?',\n",
              " 'What is the significance of switching up your work environment according to the context information?',\n",
              " 'How can perfectionism hinder productivity, as explained in the text?',\n",
              " 'What strategy is suggested to focus on progress over perfection when working on tasks or projects?',\n",
              " 'How can setting a timebox for a specific task help in completing it efficiently?',\n",
              " 'What is one of the most insidious beliefs that limits productivity according to Gratias?',\n",
              " 'How does perfectionism hinder progress and productivity in the workplace?',\n",
              " 'How can you stop obsessing over perfection and focus on progress instead?',\n",
              " 'What is the benefit of setting a timebox for tasks or projects?',\n",
              " 'Why is it important to embrace transparency and foster a sense of belonging in the workplace?',\n",
              " 'How can forming connections and having fun at work contribute to employee health and happiness?',\n",
              " 'What are some ways to boost productivity levels at work according to the article?',\n",
              " 'How does the tactic of setting a timebox instill a sense of urgency in completing tasks?',\n",
              " 'Why is it important to focus on progress rather than perfection when working on tasks or projects?',\n",
              " 'How can high expectations and the best intentions sometimes hinder productivity at work?',\n",
              " 'What is the purpose of using embeddings in LlamaIndex?',\n",
              " 'How do embedding models represent text in a numerical form?',\n",
              " 'What methods are commonly used to calculate the similarity between embeddings in LlamaIndex?',\n",
              " 'How does LlamaIndex handle embedding models that are not already installed?',\n",
              " 'What is the default embedding model used by LlamaIndex, and how can it be changed?',\n",
              " 'How are embedding models specified and used in LlamaIndex for indexing and querying?',\n",
              " 'What are some considerations for users when choosing to use a local model for embeddings in LlamaIndex?',\n",
              " 'How does LlamaIndex support the creation and usage of ONNX embeddings?',\n",
              " \"How can users extend LlamaIndex's base embeddings class to implement their own embeddings?\",\n",
              " 'How do Instructor Embeddings differ from other embedding models, and in what scenarios are they particularly useful?',\n",
              " 'What is the purpose of LlamaIndex in the context of Large Language Models (LLM)?',\n",
              " \"How does Retrieval Augmented Generation (RAG) help LLMs answer questions based on data they weren't originally trained on?\",\n",
              " 'What are the 5 built-in Response Modes of LlamaIndex and how do they differ from each other?',\n",
              " 'How does the retrieval step in RAG work in identifying relevant data chunks from a large dataset like the Warren Report?',\n",
              " 'Why is it necessary to break down and sequence the results of multiple LLM calls when utilizing RAG techniques?',\n",
              " 'How does the compact Response Mode in LlamaIndex handle the retrieval and generation steps in answering a query?',\n",
              " 'What is the significance of setting the similarity_top_k parameter to 5 when initializing the query_engine in LlamaIndex?',\n",
              " 'How does the compact Response Mode differ from the other Response Modes in terms of processing and presenting the retrieved data chunks?',\n",
              " 'What challenges may arise when trying to fit all retrieved data chunks within the input token limit for a single LLM call?',\n",
              " 'How can LlamaIndex Response Modes be utilized to effectively combine the results of multiple LLM calls and provide a comprehensive answer to a query?',\n",
              " 'What are the 5 basic Response Modes that can be used when querying with LlamaIndex?',\n",
              " 'How does the compact Response Mode work in LlamaIndex?',\n",
              " 'Why was the compact Response Mode unable to answer the question about the conclusions of the Warren Report?',\n",
              " 'How does the refine Response Mode differ from the compact Response Mode in LlamaIndex?',\n",
              " 'What is the tree_summarize Response Mode in LlamaIndex and how does it work?',\n",
              " 'What is the purpose of setting the similarity_top_k parameter to 5 in LlamaIndex?',\n",
              " 'How does LlamaIndex handle the concatenation of chunks of retrieved data for LLM calls in the tree_summarize Response Mode?',\n",
              " 'What is the significance of the refine_template prompt in LlamaIndex?',\n",
              " 'How does the structure of the refine_template prompt help steer the LLM back onto the right track in answering queries?',\n",
              " \"How does the tree_summarize Response Mode compare to LangChain's MapReduceDocumentChain in terms of functionality?\",\n",
              " 'What is the recursive property of the tree_summarize Response Mode?',\n",
              " 'How many LLM calls were required for the tree_summarize mode in the analysis of the Warren Report?',\n",
              " 'How does the tree_summarize Response Mode differ from the refine and compact modes in terms of analyzing data chunks?',\n",
              " 'What is the main difference between the accumulate and tree_summarize Response Modes in terms of providing answers?',\n",
              " 'How does the accumulate Response Mode handle the answers returned by the LLM for chunks 1, 2, and 3?',\n",
              " 'What is the main characteristic of the simple_summarize Response Mode in terms of processing text chunks?',\n",
              " 'Why is it mentioned that the answer provided by simple_summarize mode may be attributed more to luck than a structural advantage?',\n",
              " 'How does the quality of the ultimate answer returned by simple_summarize mode change as the number of retrieved chunks increases?',\n",
              " 'What is the main advantage of the tree_summarize Response Mode over the accumulate and simple_summarize modes in providing comprehensive answers?',\n",
              " 'Based on the results of the tests mentioned, which Response Mode returned the most comprehensive and complete answer to the question about the conclusions of the Warren Report?',\n",
              " 'What are the two Response Modes discussed in the context, and how do they differ in terms of providing comprehensive answers?',\n",
              " 'How does the simple_summarize mode handle an increase in the number of chunks retrieved during the retrieval step?',\n",
              " 'What advantages does the tree_summarize Response Mode offer when summarizing content from a large body of text?',\n",
              " 'Why is it important to understand the different motivations behind each of the Response Modes discussed in the context?',\n",
              " 'How does the LlamaIndex framework compare to LangChain in terms of building RAG-style applications?',\n",
              " 'Who is Bobby Gill, and what role does he play at BlueLabel?',\n",
              " 'What is the purpose of the BlueLabel AI/ML practice led by Bobby Gill?',\n",
              " 'How does the LlamaIndex documentation help developers interested in building with it?',\n",
              " 'In what ways does the team at BlueLabel emphasize the importance of maximizing the efficiency of each token used in LLM calls?',\n",
              " 'How does the context suggest that the need for RAG may diminish in a world where LLMs can accept million token inputs?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# template = (\n",
        "#         \"Context information is below. \\n\"\n",
        "#         \"---------------------\\n\"\n",
        "#         \"{context_str}\"\n",
        "#         \"\\n---------------------\\n\"\n",
        "#         \"Given the context information and not prior knowledge, answer the query.\"\n",
        "#         \"{query_str}\\n\"\n",
        "#         \"Provide only the URLs of all the websites that answer the query. If none of the websites answer the query, return 'None'.\"\n",
        "#         \"Answer:\"\n",
        "#         )\n",
        "\n",
        "# qa_template = PromptTemplate(template)"
      ],
      "metadata": {
        "id": "E3vyLjMAv8PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
        "from llama_index.core.postprocessor import LLMRerank\n",
        "# question = \"Which website mentioned how the tree summarize mode differs from the refine response mode?\"\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\") # did not use it gives error\n",
        "evaluator_gpt = RelevancyEvaluator()\n",
        "# index = VectorStoreIndex.from_documents(documents)\n",
        "# query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)])\n",
        "index = VectorStoreIndex(sentence_window_node_parser_nodes)\n",
        "# query_engine = index.as_query_engine(similarity_top_k=15, node_postprocessors=[MetadataReplacementPostProcessor(target_metadata_key=\"window\"), LLMRerank(top_n=7,)])\n",
        "query_engine = index.as_query_engine(similarity_top_k = 20, node_postprocessors=[MetadataReplacementPostProcessor(target_metadata_key=\"window\"), SimilarityPostprocessor(similarity_cutoff=0.75)])\n",
        "response_vector = query_engine.query(eval_questions[1])\n",
        "eval_result = evaluator_gpt.evaluate_response(query=eval_questions[1], response=response_vector)"
      ],
      "metadata": {
        "id": "d3cpGzEKt-Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_questions[1])\n",
        "print(str(response_vector))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtKgRcSaxOWC",
        "outputId": "e6ff3782-ef98-4499-a0ac-19fc3c48ded1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How do RL algorithms use a reward-and-punishment paradigm to process data?\n",
            "RL algorithms use a reward-and-punishment paradigm to process data by learning from the feedback of each action. Positive actions that work towards the goal are reinforced through rewards, while negative actions that detract from the goal are ignored or punished. This feedback mechanism allows the algorithms to self-discover the best processing paths to achieve final outcomes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_nodes = response_vector.source_nodes\n",
        "\n",
        "for node in source_nodes:\n",
        "  print(\"**\")\n",
        "  print(node.get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SoWfQbz5fH9",
        "outputId": "60af6f9e-dc8d-42f3-b1a2-d9cd01085fac",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**\n",
            "Explore AWS Skill Builder | Access hundreds of free digital courses, wherever, whenever you want » Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results.  It mimics the trial-and-error learning process that humans use to achieve their goals.  Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored.  RL algorithms use a reward-and-punishment paradigm as they process data.  They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.  The algorithms are also capable of delayed gratification.  The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. \n",
            "**\n",
            "It makes decisions based on factors like current and available cloud infrastructure, spending, and utilization.  The dynamics of financial markets are complex, with statistical properties that change over time.  RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts.  For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards.  It dynamically creates a value function and develops a strategy to maximize profits.  The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology.  For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell. \n",
            "**\n",
            "For example, a cloud spend optimizing system uses RL to adjust to fluctuating resource needs and choose optimal instance types, quantities, and configurations.  It makes decisions based on factors like current and available cloud infrastructure, spending, and utilization.  The dynamics of financial markets are complex, with statistical properties that change over time.  RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts.  For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards.  It dynamically creates a value function and develops a strategy to maximize profits.  The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology. \n",
            "**\n",
            "The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology.  For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell.  Soon, the child learns which combination of activities results in the end reward.  An RL algorithm mimics a similar learning process.  It tries different activities to learn the associated negative and positive values to achieve the end reward outcome.  In reinforcement learning, there are a few key concepts to familiarize yourself with: Reinforcement learning is based on the Markov decision process, a mathematical modeling of decision-making that uses discrete time steps.  At every step, the agent takes a new action that results in a new environment state. \n",
            "**\n",
            "When you use an RL algorithm, this isn’t necessary.  It learns by itself.  At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections.  RL inherently focuses on long-term reward maximization, which makes it apt for scenarios where actions have prolonged consequences.  It is particularly well-suited for real-world situations where feedback isn't immediately available for every step, since it can learn from delayed rewards.  For example, decisions about energy consumption or storage might have long-term consequences.  RL can be used to optimize long-term energy efficiency and cost. \n",
            "**\n",
            "RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts.  For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards.  It dynamically creates a value function and develops a strategy to maximize profits.  The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology.  For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell.  Soon, the child learns which combination of activities results in the end reward.  An RL algorithm mimics a similar learning process. \n",
            "**\n",
            "All these algorithms can be grouped into two broad categories.  Model-based RL is typically used when environments are well-defined and unchanging and where real-world environment testing is difficult.  The agent first builds an internal representation (model) of the environment.  It uses this process to build this model: Once the model is complete, the agent simulates action sequences based on the probability of optimal cumulative rewards.  It then further assigns values to the action sequences themselves.  The agent thus develops different strategies within the environment to achieve the desired end goal.  Consider a robot learning to navigate a new building to reach a specific room. \n",
            "**\n",
            "It mimics the trial-and-error learning process that humans use to achieve their goals.  Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored.  RL algorithms use a reward-and-punishment paradigm as they process data.  They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.  The algorithms are also capable of delayed gratification.  The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way.  RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments. \n",
            "**\n",
            "For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards.  It dynamically creates a value function and develops a strategy to maximize profits.  The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology.  For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell.  Soon, the child learns which combination of activities results in the end reward.  An RL algorithm mimics a similar learning process.  It tries different activities to learn the associated negative and positive values to achieve the end reward outcome. \n",
            "**\n",
            "Read about supervised and unsupervised learning » In supervised learning, you define both the input and the expected associated output.  For instance, you can provide a set of images labeled dogs or cats, and the algorithm is then expected to identify a new animal image as a dog or cat.  Supervised learning algorithms learn patterns and relationships between the input and output pairs.  Then, they predict outcomes based on new input data.  It requires a supervisor, typically a human, to label each data record in a training data set with an output.  In contrast, RL has a well-defined end goal in the form of a desired result but no supervisor to label associated data in advance.  During training, instead of trying to map inputs with known outputs, it maps inputs with possible outcomes. \n",
            "**\n",
            "It requires a supervisor, typically a human, to label each data record in a training data set with an output.  In contrast, RL has a well-defined end goal in the form of a desired result but no supervisor to label associated data in advance.  During training, instead of trying to map inputs with known outputs, it maps inputs with possible outcomes.  By rewarding desired behaviors, you give weightage to the best outcomes.  Unsupervised learning algorithms receive inputs with no specified outputs during the training process.  They find hidden patterns and relationships within the data using statistical means.  For instance, you could provide a set of documents, and the algorithm may group them into categories it identifies based on the words in the text. \n",
            "**\n",
            "The policies help it decide which action to take next for optimal cumulative reward.  The agent must also choose between further environment exploration to learn new state-action rewards or select known high-reward actions from a given state.  This is called the exploration-exploitation trade-off .  There are various algorithms used in reinforcement learning (RL)—such as Q-learning, policy gradient methods, Monte Carlo methods, and temporal difference learning.  Deep RL is the application of deep neural networks to reinforcement learning.  One example of a deep RL algorithm is Trust Region Policy Optimization (TRPO).  All these algorithms can be grouped into two broad categories. \n",
            "**\n",
            "When we have unclassified and unlabeled data, the system attempts to uncover patterns from the data .  There is no label or target given for the examples.  One common task is to group similar examples together called clustering .  Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps.  This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance.  Simple reward feedback is required for the agent to learn which action is best.  This is known as the reinforcement signal. \n",
            "**\n",
            "RL algorithms can be used in complex environments with many rules and dependencies.  In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment.  Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results.  In traditional ML algorithms, humans must label data pairs to direct the algorithm.  When you use an RL algorithm, this isn’t necessary.  It learns by itself.  At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections. \n",
            "**\n",
            "It can make it harder for the algorithm to be effective in practice.  Like any field of science, data science also looks at conclusive research and findings to establish standards and procedures.  Data scientists prefer knowing how a specific conclusion was reached for provability and replication.  With complex RL algorithms, the reasons why a particular sequence of steps was taken may be difficult to ascertain.  Which actions in a sequence were the ones that led to the optimal end result?  This can be difficult to deduce, which causes implementation challenges.  Amazon Web Services (AWS) has many offerings that help you develop, train, and deploy reinforcement learning (RL) algorithms for real-world applications. \n",
            "**\n",
            "One common task is to group similar examples together called clustering .  Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps.  This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance.  Simple reward feedback is required for the agent to learn which action is best.  This is known as the reinforcement signal.  For example, maximizing the points won in a game over a lot of moves.  Regression is a technique used to predict the value of response (dependent) variables from one or more predictor (independent) variables. \n",
            "**\n",
            "RL algorithms use a reward-and-punishment paradigm as they process data.  They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.  The algorithms are also capable of delayed gratification.  The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way.  RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments.  There are many benefits to using reinforcement learning (RL).  However, these three often stand out. \n",
            "**\n",
            "Consider a self-driving car that needs to navigate city traffic.  Roads, traffic patterns, pedestrian behavior, and countless other factors can make the environment highly dynamic and complex.  AI teams train the vehicle in a simulated environment in the initial stages.  The vehicle takes actions based on its current state and receives rewards or penalties.  Over time, by driving millions of miles in different virtual scenarios, the vehicle learns which actions are best for each state without explicitly modeling the entire traffic dynamics.  When introduced in the real world, the vehicle uses the learned policy but continues to refine it with new data.  While supervised learning, unsupervised learning, and reinforcement learning (RL) are all ML algorithms in the field of AI, there are distinctions between the three. \n",
            "**\n",
            "RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments.  There are many benefits to using reinforcement learning (RL).  However, these three often stand out.  RL algorithms can be used in complex environments with many rules and dependencies.  In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment.  Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results.  In traditional ML algorithms, humans must label data pairs to direct the algorithm. \n",
            "**\n",
            "Soon, the child learns which combination of activities results in the end reward.  An RL algorithm mimics a similar learning process.  It tries different activities to learn the associated negative and positive values to achieve the end reward outcome.  In reinforcement learning, there are a few key concepts to familiarize yourself with: Reinforcement learning is based on the Markov decision process, a mathematical modeling of decision-making that uses discrete time steps.  At every step, the agent takes a new action that results in a new environment state.  Similarly, the current state is attributed to the sequence of previous actions.  Through trial and error in moving through the environment, the agent builds a set of if-then rules or policies. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_vector.source_nodes[0].node.get_content()[:1000] + \"...\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "c1iLOHJHzHDT",
        "outputId": "e96fcd03-110c-403f-b302-870169d1bc9e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Explore AWS Skill Builder | Access hundreds of free digital courses, wherever, whenever you want » Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results.  It mimics the trial-and-error learning process that humans use to achieve their goals.  Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored.  RL algorithms use a reward-and-punishment paradigm as they process data.  They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.  The algorithms are also capable of delayed gratification.  The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. ...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "returns: query, context, response, passing, feedback, score, pairwise_source, invalid_result, invalid_reason"
      ],
      "metadata": {
        "id": "4zrb2ie3bwlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQEfU8nfy6nE",
        "outputId": "baa89aad-bf9f-4c4e-ee76-b6568d4143e6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(query='How do RL algorithms use a reward-and-punishment paradigm to process data?', contexts=['Explore AWS Skill Builder | Access hundreds of free digital courses, wherever, whenever you want » Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results.  It mimics the trial-and-error learning process that humans use to achieve their goals.  Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored.  RL algorithms use a reward-and-punishment paradigm as they process data.  They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.  The algorithms are also capable of delayed gratification.  The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. ', 'It makes decisions based on factors like current and available cloud infrastructure, spending, and utilization.  The dynamics of financial markets are complex, with statistical properties that change over time.  RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts.  For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards.  It dynamically creates a value function and develops a strategy to maximize profits.  The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology.  For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell. ', 'For example, a cloud spend optimizing system uses RL to adjust to fluctuating resource needs and choose optimal instance types, quantities, and configurations.  It makes decisions based on factors like current and available cloud infrastructure, spending, and utilization.  The dynamics of financial markets are complex, with statistical properties that change over time.  RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts.  For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards.  It dynamically creates a value function and develops a strategy to maximize profits.  The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology. ', 'The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology.  For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell.  Soon, the child learns which combination of activities results in the end reward.  An RL algorithm mimics a similar learning process.  It tries different activities to learn the associated negative and positive values to achieve the end reward outcome.  In reinforcement learning, there are a few key concepts to familiarize yourself with: Reinforcement learning is based on the Markov decision process, a mathematical modeling of decision-making that uses discrete time steps.  At every step, the agent takes a new action that results in a new environment state. ', \"When you use an RL algorithm, this isn’t necessary.  It learns by itself.  At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections.  RL inherently focuses on long-term reward maximization, which makes it apt for scenarios where actions have prolonged consequences.  It is particularly well-suited for real-world situations where feedback isn't immediately available for every step, since it can learn from delayed rewards.  For example, decisions about energy consumption or storage might have long-term consequences.  RL can be used to optimize long-term energy efficiency and cost. \", 'RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts.  For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards.  It dynamically creates a value function and develops a strategy to maximize profits.  The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology.  For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell.  Soon, the child learns which combination of activities results in the end reward.  An RL algorithm mimics a similar learning process. ', 'All these algorithms can be grouped into two broad categories.  Model-based RL is typically used when environments are well-defined and unchanging and where real-world environment testing is difficult.  The agent first builds an internal representation (model) of the environment.  It uses this process to build this model: Once the model is complete, the agent simulates action sequences based on the probability of optimal cumulative rewards.  It then further assigns values to the action sequences themselves.  The agent thus develops different strategies within the environment to achieve the desired end goal.  Consider a robot learning to navigate a new building to reach a specific room. ', 'It mimics the trial-and-error learning process that humans use to achieve their goals.  Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored.  RL algorithms use a reward-and-punishment paradigm as they process data.  They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.  The algorithms are also capable of delayed gratification.  The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way.  RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments. ', 'For instance, an algorithm could observe the rules and patterns of the stock market before it tests actions and records associated rewards.  It dynamically creates a value function and develops a strategy to maximize profits.  The learning process of reinforcement learning (RL) algorithms is similar to animal and human reinforcement learning in the field of behavioral psychology.  For instance, a child may discover that they receive parental praise when they help a sibling or clean but receive negative reactions when they throw toys or yell.  Soon, the child learns which combination of activities results in the end reward.  An RL algorithm mimics a similar learning process.  It tries different activities to learn the associated negative and positive values to achieve the end reward outcome. ', 'Read about supervised and unsupervised learning » In supervised learning, you define both the input and the expected associated output.  For instance, you can provide a set of images labeled dogs or cats, and the algorithm is then expected to identify a new animal image as a dog or cat.  Supervised learning algorithms learn patterns and relationships between the input and output pairs.  Then, they predict outcomes based on new input data.  It requires a supervisor, typically a human, to label each data record in a training data set with an output.  In contrast, RL has a well-defined end goal in the form of a desired result but no supervisor to label associated data in advance.  During training, instead of trying to map inputs with known outputs, it maps inputs with possible outcomes. ', 'It requires a supervisor, typically a human, to label each data record in a training data set with an output.  In contrast, RL has a well-defined end goal in the form of a desired result but no supervisor to label associated data in advance.  During training, instead of trying to map inputs with known outputs, it maps inputs with possible outcomes.  By rewarding desired behaviors, you give weightage to the best outcomes.  Unsupervised learning algorithms receive inputs with no specified outputs during the training process.  They find hidden patterns and relationships within the data using statistical means.  For instance, you could provide a set of documents, and the algorithm may group them into categories it identifies based on the words in the text. ', 'The policies help it decide which action to take next for optimal cumulative reward.  The agent must also choose between further environment exploration to learn new state-action rewards or select known high-reward actions from a given state.  This is called the exploration-exploitation trade-off .  There are various algorithms used in reinforcement learning (RL)—such as Q-learning, policy gradient methods, Monte Carlo methods, and temporal difference learning.  Deep RL is the application of deep neural networks to reinforcement learning.  One example of a deep RL algorithm is Trust Region Policy Optimization (TRPO).  All these algorithms can be grouped into two broad categories. ', 'When we have unclassified and unlabeled data, the system attempts to uncover patterns from the data .  There is no label or target given for the examples.  One common task is to group similar examples together called clustering .  Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps.  This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance.  Simple reward feedback is required for the agent to learn which action is best.  This is known as the reinforcement signal. ', 'RL algorithms can be used in complex environments with many rules and dependencies.  In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment.  Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results.  In traditional ML algorithms, humans must label data pairs to direct the algorithm.  When you use an RL algorithm, this isn’t necessary.  It learns by itself.  At the same time, it offers mechanisms to integrate human feedback, allowing for systems that adapt to human preferences, expertise, and corrections. ', 'It can make it harder for the algorithm to be effective in practice.  Like any field of science, data science also looks at conclusive research and findings to establish standards and procedures.  Data scientists prefer knowing how a specific conclusion was reached for provability and replication.  With complex RL algorithms, the reasons why a particular sequence of steps was taken may be difficult to ascertain.  Which actions in a sequence were the ones that led to the optimal end result?  This can be difficult to deduce, which causes implementation challenges.  Amazon Web Services (AWS) has many offerings that help you develop, train, and deploy reinforcement learning (RL) algorithms for real-world applications. ', 'One common task is to group similar examples together called clustering .  Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps.  This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance.  Simple reward feedback is required for the agent to learn which action is best.  This is known as the reinforcement signal.  For example, maximizing the points won in a game over a lot of moves.  Regression is a technique used to predict the value of response (dependent) variables from one or more predictor (independent) variables. ', 'RL algorithms use a reward-and-punishment paradigm as they process data.  They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.  The algorithms are also capable of delayed gratification.  The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way.  RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments.  There are many benefits to using reinforcement learning (RL).  However, these three often stand out. ', 'Consider a self-driving car that needs to navigate city traffic.  Roads, traffic patterns, pedestrian behavior, and countless other factors can make the environment highly dynamic and complex.  AI teams train the vehicle in a simulated environment in the initial stages.  The vehicle takes actions based on its current state and receives rewards or penalties.  Over time, by driving millions of miles in different virtual scenarios, the vehicle learns which actions are best for each state without explicitly modeling the entire traffic dynamics.  When introduced in the real world, the vehicle uses the learned policy but continues to refine it with new data.  While supervised learning, unsupervised learning, and reinforcement learning (RL) are all ML algorithms in the field of AI, there are distinctions between the three. ', 'RL is a powerful method to help artificial intelligence (AI) systems achieve optimal outcomes in unseen environments.  There are many benefits to using reinforcement learning (RL).  However, these three often stand out.  RL algorithms can be used in complex environments with many rules and dependencies.  In the same environment, a human may not be capable of determining the best path to take, even with superior knowledge of the environment.  Instead, model-free RL algorithms adapt quickly to continuously changing environments and find new strategies to optimize results.  In traditional ML algorithms, humans must label data pairs to direct the algorithm. ', 'Soon, the child learns which combination of activities results in the end reward.  An RL algorithm mimics a similar learning process.  It tries different activities to learn the associated negative and positive values to achieve the end reward outcome.  In reinforcement learning, there are a few key concepts to familiarize yourself with: Reinforcement learning is based on the Markov decision process, a mathematical modeling of decision-making that uses discrete time steps.  At every step, the agent takes a new action that results in a new environment state.  Similarly, the current state is attributed to the sequence of previous actions.  Through trial and error in moving through the environment, the agent builds a set of if-then rules or policies. '], response='RL algorithms use a reward-and-punishment paradigm to process data by learning from the feedback of each action. Positive actions that work towards the goal are reinforced through rewards, while negative actions that detract from the goal are ignored or punished. This feedback mechanism allows the algorithms to self-discover the best processing paths to achieve final outcomes.', passing=True, feedback='YES', score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import EvaluationResult\n",
        "\n",
        "# define jupyter display function\n",
        "def display_eval_df(\n",
        "    query: str, response: Response, eval_result: EvaluationResult\n",
        ") -> None:\n",
        "    eval_df = pd.DataFrame(\n",
        "        {\n",
        "            \"Query\": query,\n",
        "            \"Response\": str(response),\n",
        "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
        "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
        "            \"Reasoning\": eval_result.feedback,\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "    eval_df = eval_df.style.set_properties(\n",
        "        **{\n",
        "            \"inline-size\": \"600px\",\n",
        "            \"overflow-wrap\": \"break-word\",\n",
        "        },\n",
        "        subset=[\"Response\", \"Source\"]\n",
        "    )\n",
        "    display(eval_df)"
      ],
      "metadata": {
        "id": "PHTWSTvksJL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_eval_df(eval_questions[1], response_vector, eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "kWULnUX2sKNI",
        "outputId": "c25c520d-cbb0-4834-fd62-0d2074f63fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7bbe28ceebc0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_526a0_row0_col1, #T_526a0_row0_col2 {\n",
              "  inline-size: 600px;\n",
              "  overflow-wrap: break-word;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_526a0\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_526a0_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
              "      <th id=\"T_526a0_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
              "      <th id=\"T_526a0_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
              "      <th id=\"T_526a0_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
              "      <th id=\"T_526a0_level0_col4\" class=\"col_heading level0 col4\" >Reasoning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_526a0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_526a0_row0_col0\" class=\"data row0 col0\" >How do RL algorithms use a reward-and-punishment paradigm to process data?</td>\n",
              "      <td id=\"T_526a0_row0_col1\" class=\"data row0 col1\" >RL algorithms use a reward-and-punishment paradigm to process data by learning from the feedback of each action. Positive actions that work towards the goal are reinforced through rewards, while negative actions that detract from the goal are ignored or punished. This feedback mechanism allows the algorithms to self-discover the best processing paths to achieve final outcomes.</td>\n",
              "      <td id=\"T_526a0_row0_col2\" class=\"data row0 col2\" >Explore AWS Skill Builder | Access hundreds of free digital courses, wherever, whenever you want » Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results.  It mimics the trial-and-error learning process that humans use to achieve their goals.  Software actions that work towards your goal are reinforced, while actions that detract from the goal are ignored.  RL algorithms use a reward-and-punishment paradigm as they process data.  They learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.  The algorithms are also capable of delayed gratification.  The best overall strategy may require short-term sacrifices, so the best approach they discover may include some punishments or backtracking along the way. ...</td>\n",
              "      <td id=\"T_526a0_row0_col3\" class=\"data row0 col3\" >Pass</td>\n",
              "      <td id=\"T_526a0_row0_col4\" class=\"data row0 col4\" >YES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import textwrap\n",
        "\n",
        "# response_str = response_vector.response\n",
        "# for source_node in response_vector.source_nodes:\n",
        "#     eval_result = evaluator_gpt.evaluate(\n",
        "#         query=question,\n",
        "#         response=response_str,\n",
        "#         contexts=[source_node.get_content()],\n",
        "#     )\n",
        "\n",
        "#     if str(eval_result.passing)  == \"True\":\n",
        "#       wrapped_text = textwrap.fill(source_node.get_content(), width=100)\n",
        "#       print(wrapped_text)\n",
        "#       print(\"\\n\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oQDTx9PWaWEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import BatchEvalRunner\n",
        "\n",
        "runner = BatchEvalRunner(\n",
        "    {\"relevancy\": evaluator_gpt},\n",
        "    workers=8,\n",
        ")\n",
        "\n",
        "eval_results = await runner.aevaluate_queries(\n",
        "    query_engine, queries=eval_questions\n",
        ")\n"
      ],
      "metadata": {
        "id": "nnMhsvqbcWkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=0\n",
        "acc=0\n",
        "fail_items=[]\n",
        "for item in eval_results['relevancy']:\n",
        "  if item.passing:\n",
        "    acc+=1\n",
        "  else:\n",
        "    fail_items.append([item.query, item.response])\n",
        "  n+=1\n",
        "print(acc, n)\n",
        "print(acc/n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQdEdkJrjrkw",
        "outputId": "79a6d892-b031-4dea-bfcb-828dfb974ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "208 220\n",
            "0.9454545454545454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fail_items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqyWzseIxpxJ",
        "outputId": "138e56df-7b58-46ef-c5f2-efbc9955f280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Why is the problem of imbalanced classes not addressed in the post on sentiment classification using Word2Vec?',\n",
              "  'The problem of imbalanced classes is not addressed in the post on sentiment classification using Word2Vec because a simple function to retrieve the top few records for each sentiment is written instead of dealing with the issue of imbalanced classes.'],\n",
              " ['How does the compact Response Mode in LlamaIndex handle the retrieval and generation steps in answering a query?',\n",
              "  'The compact Response Mode in LlamaIndex concatenates multiple retrieved text chunks into the largest possible string that fits into a single prompt for the LLM. It uses this concatenated string to make calls to the LLM for answering a query. If the concatenated chunks do not contain the necessary information to answer the query coherently, the compact Response Mode may struggle to provide a meaningful response.'],\n",
              " ['How does the compact Response Mode differ from the other Response Modes in terms of processing and presenting the retrieved data chunks?',\n",
              "  'The compact Response Mode differs from the other Response Modes by concatenating multiple retrieved data chunks into a single prompt to the LLM, maximizing the use of the LLM token limit. This mode requires multiple LLM calls to process and present the retrieved data chunks, which can lead to a less coherent answer compared to other Response Modes that handle the chunks differently in terms of processing and presenting the information to the LLM.'],\n",
              " ['What are the 5 basic Response Modes that can be used when querying with LlamaIndex?',\n",
              "  'The 5 basic Response Modes that can be used when querying with LlamaIndex are compact, refine, tree_summarize, accumulation, and simple_summarize.'],\n",
              " ['How does the compact Response Mode work in LlamaIndex?',\n",
              "  'The compact Response Mode in LlamaIndex concatenates as many retrieved text chunks as possible into a single prompt to the GPT-4 LLM. If the concatenated chunks exceed the context window for a single GPT-4 call, multiple LLM calls are made to answer the query.'],\n",
              " ['Why was the compact Response Mode unable to answer the question about the conclusions of the Warren Report?',\n",
              "  'The compact Response Mode was unable to answer the question about the conclusions of the Warren Report because the first 4 chunks of text from the report did not contain the conclusions. The compact Response Mode concatenated these initial chunks for a single prompt to the LLM, which led to an incoherent answer. The actual conclusions were found in Chunk 5, which had a lower Cosine similarity score than the preceding chunks, making it challenging for the compact Response Mode to steer the LLM back on track to provide a proper answer.'],\n",
              " ['How does the refine Response Mode differ from the compact Response Mode in LlamaIndex?',\n",
              "  'The refine Response Mode in LlamaIndex differs from the compact Response Mode in that it includes only 1 chunk of retrieved data for each LLM call, progressing sequentially through each remaining chunk one at a time. On the other hand, the compact Response Mode concatenates as many chunks as possible to maximize the use of the LLM token limit, attempting to build a larger string for a single prompt to the GPT-4 LLM.'],\n",
              " ['How does the structure of the refine_template prompt help steer the LLM back onto the right track in answering queries?',\n",
              "  'The structure of the refine_template prompt helps steer the LLM back onto the right track in answering queries by allowing it to build upon the answer returned from the previous chunk. This sequential progression through each chunk, one at a time, enables the refine Response Mode to refine and enhance the answer with each subsequent LLM call. This approach aims to overcome the challenge of providing a coherent answer when the initial chunks passed to the LLM are less relevant, ultimately guiding the LLM towards a more accurate response.'],\n",
              " ['What is the main difference between the accumulate and tree_summarize Response Modes in terms of providing answers?',\n",
              "  'The main difference between the accumulate and tree_summarize Response Modes in terms of providing answers is that the accumulate mode returns multiple answers scoped to each context chunk passed to the LLM in separate calls, while the tree_summarize mode uses a map-reduce pattern to independently analyze and combine concatenated chunks of data into a single answer in a separate prompt.'],\n",
              " ['How does the accumulate Response Mode handle the answers returned by the LLM for chunks 1, 2, and 3?',\n",
              "  \"The accumulate Response Mode returns n answers for each chunk passed to the LLM in a call. For chunks 1, 2, and 3, the LLM returned responses equivalent to 'The context does not provide information on the main conclusions of the Warren Report regarding the assassination of President Kennedy,' which are not included in the final result.\"],\n",
              " ['How does the quality of the ultimate answer returned by simple_summarize mode change as the number of retrieved chunks increases?',\n",
              "  'The quality of the ultimate answer returned by simple_summarize mode decreases as the number of retrieved chunks increases.'],\n",
              " ['How does the simple_summarize mode handle an increase in the number of chunks retrieved during the retrieval step?',\n",
              "  'As the number of chunks retrieved during the retrieval step increases in the simple_summarize mode, the quality of the ultimate answer returned is likely to decrease. This decrease in quality occurs because the knowledge is lost in the truncated segments of each chunk, leading to a potential decrease in the overall quality of the final answer.']]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}